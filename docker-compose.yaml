version: "3.7"

services:
  # Use `docker compose up backend-dev --build --attach-dependencies` to start a database and work and the backend.
  backend-dev:
    image: sverrirab/sleep
    depends_on: [db, adminer, redis, redis-insights]

  # Use `docker compose up frontend-dev --build --attach-dependencies` to start all services needed to work on the frontend.
  frontend-dev:
    image: sverrirab/sleep
    depends_on: [db, webdb, adminer, maildev, backend, redis]

  # Used by CI automations.
  ci:
    image: sverrirab/sleep
    depends_on: [db, webdb, maildev, backend, redis, web]

  # This DB is for the FastAPI Backend.
  db:
    image: ghcr.io/laion-ai/open-assistant/oasst-postgres
    pull_policy: always
    restart: always
    ports:
      - 5432:5432
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: postgres
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "postgres"]
      interval: 2s
      timeout: 2s
      retries: 10

  # Redis - caching + rate limiting on BE
  redis:
    image: redis
    restart: always
    ports:
      - 6379:6379
    healthcheck:
      test: ["CMD-SHELL", "redis-cli ping | grep PONG"]
      interval: 2s
      timeout: 2s
      retries: 10
    command: redis-server /usr/local/etc/redis/redis.conf
    volumes:
      - ./redis.conf:/usr/local/etc/redis/redis.conf
  # insights host - redis:6379
  redis-insights:
    image: redislabs/redisinsight:latest
    ports:
      - 8001:8001

  # This DB is for Web Authentication and data caching.
  webdb:
    image: postgres
    restart: always
    ports:
      - 5433:5432
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: oasst_web
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "postgres"]
      interval: 2s
      timeout: 2s
      retries: 10

  # This lets you manually inspect the web and backend databases.
  adminer:
    image: adminer
    restart: always
    ports:
      - 8089:8080

  # This fakes an SMTP email server used by website authentication.
  # User registration emails can be found by going to localhost:1080 and
  # opening the emails listed.
  maildev:
    image: maildev/maildev
    restart: always
    environment:
      - MAILDEV_WEB_PORT=1080
      - MAILDEV_SMTP_PORT=1025
    ports:
      - "1080:1080"
      - "1025:1025"

  # The oassist backend service.
  backend:
    build:
      dockerfile: docker/Dockerfile.backend
      context: .
    image: oasst-backend
    environment:
      - POSTGRES_HOST=db
      - REDIS_HOST=redis
      - DEBUG_USE_SEED_DATA=True
      - DEBUG_ALLOW_SELF_LABELING=True
      - MAX_WORKERS=1
      - DEBUG_SKIP_TOXICITY_CALCULATION=True
      - DEBUG_SKIP_EMBEDDING_COMPUTATION=True
    depends_on:
      db:
        condition: service_healthy
    ports:
      - "8080:8080"

  # The oassist web service.
  web:
    build:
      dockerfile: docker/Dockerfile.website
      context: .
    image: oasst-web
    environment:
      - DATABASE_URL=postgres://postgres:postgres@webdb/oasst_web
      - FASTAPI_URL=http://backend:8080
      - FASTAPI_KEY=1234
      - NEXTAUTH_SECRET=O/M2uIbGj+lDD2oyNa8ax4jEOJqCPJzO53UbWShmq98=
      - EMAIL_SERVER_HOST=maildev
      - EMAIL_SERVER_PORT=1025
      - EMAIL_FROM=info@example.com
      - NEXTAUTH_URL=http://localhost:3000
      - DEBUG_LOGIN=true
      - NEXT_PUBLIC_CLOUDFARE_CAPTCHA_SITE_KEY=1x00000000000000000000AA
      - CLOUDFLARE_CAPTCHA_SECRET_KEY=1x0000000000000000000000000000000AA
      - NEXT_PUBLIC_ENABLE_EMAIL_SIGNIN_CAPTCHA=true
      - NEXT_PUBLIC_ENABLE_EMAIL_SIGNIN=true
    depends_on:
      webdb:
        condition: service_healthy
    ports:
      - "3000:3000"
    command: bash wait-for-postgres.sh node server.js

  inference-server:
    build:
      dockerfile: docker/inference/Dockerfile.server
      context: .
      target: dev
    image: oasst-inference-server:dev
    environment:
      - "PORT=8000"
      - "REDIS_HOST=redis"
    volumes:
      - "./oasst-shared:/opt/inference/lib/oasst-shared"
      - "./inference/server:/opt/inference/server"
    restart: unless-stopped
    depends_on:
      redis:
        condition: service_healthy
    profiles: ["inference"]

  inference-worker:
    build:
      dockerfile: docker/inference/Dockerfile.worker
      context: .
      target: dev
    image: oasst-inference-worker:dev
    environment:
      - "BACKEND_URL=ws://inference-server:8000"
      - "INFERENCE_SERVER_URL=http://inference-text-generation-server"
    volumes:
      - "./oasst-shared:/opt/inference/lib/oasst-shared"
      - "./inference/worker:/opt/inference/worker"
    depends_on:
      - inference-server
    deploy:
      replicas: 1
    profiles: ["inference"]

  inference-text-client:
    build:
      dockerfile: docker/inference/Dockerfile.text-client
      context: .
    image: oasst-inference-text-client
    environment:
      - "BACKEND_URL=http://inference-server:8000"
    tty: true
    stdin_open: true
    volumes:
      - "./inference/worker:/opt/inference/worker"
    restart: unless-stopped
    depends_on:
      - inference-server
    profiles: ["inference"]

  inference-text-generation-server:
    image: ghcr.io/huggingface/text-generation-inference
    environment:
      - "MODEL_ID=distilgpt2"
    profiles: ["inference"]
