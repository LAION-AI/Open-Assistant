{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "435c534a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset,Dataset\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "SBERT_MODEL = \"all-MiniLM-L6-v2\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.spatial as sp\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import faiss\n",
    "from collections import ChainMap\n",
    "from scipy.special import softmax\n",
    "import torch\n",
    "np.set_printoptions(precision=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c431d120",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "THRESHOLD = 0.72\n",
    "B_SIZE = 1000\n",
    "MAXLEN = 196\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9e2aaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vectorizer(model=SBERT_MODEL):\n",
    "    return SentenceTransformer(model)\n",
    "\n",
    "\n",
    "def vectorize_text(model, texts):\n",
    "    return model.encode(texts, show_progress_bar=True)\n",
    "\n",
    "def build_index(vector):\n",
    "    \n",
    "    d = vector.shape[1]\n",
    "    index = faiss.IndexFlatIP(d)\n",
    "    faiss.normalize_L2(rot_vector)\n",
    "    index.add(rot_vector)\n",
    "    return index\n",
    "    \n",
    "\n",
    "def get_conversations(rot_vector, posts_vector):\n",
    "    \n",
    "    index = build_index(rot_vector)\n",
    "    num_result=5\n",
    "    faiss.normalize_L2(posts_vector)\n",
    "    matching_dict = defaultdict(list)\n",
    "    for idx in tqdm(range(0, posts_vector.shape[0], B_SIZE)):\n",
    "        I, D = index.search(posts_vector[idx:idx+B_SIZE],num_result)\n",
    "        sim_indices = np.argwhere(I >= THRESHOLD)\n",
    "        #return sim_indices\n",
    "        for k, j in sim_indices:\n",
    "            matching_dict[k+idx].append({D[k][j]:I[k][j]})\n",
    "\n",
    "    return matching_dict\n",
    "\n",
    "def get_top_n(data,n=2):\n",
    "    \n",
    "    data = dict(ChainMap(*data))\n",
    "    data = {k: v for k, v in sorted(data.items(), key=lambda data: data[1],reverse=True)}\n",
    "    return list(data.keys())[:n]\n",
    "    \n",
    "\n",
    "def convert_to_json(match_dict,all_rots,reddit_df):\n",
    "    all_data = []\n",
    "    posts = reddit_df['title'].values\n",
    "    permalinks = reddit_df['permalink'].values\n",
    "    for post_id,rot_list in match_dict.items():\n",
    "        rot_ids = get_top_n(rot_list)\n",
    "        ps_rots = [all_rots[idx] for idx in rot_ids]\n",
    "        all_data.append({\"context\":posts[post_id], \"rots\":ps_rots,\"permalink\":permalinks[post_id],\"episone_done\":True})\n",
    "    \n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f089487b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration allenai--prosocial-dialog-ebbad39ca08b6d44\n",
      "Found cached dataset json (/home/shahul/.cache/huggingface/datasets/allenai___json/allenai--prosocial-dialog-ebbad39ca08b6d44/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n",
      "/tmp/ipykernel_142180/1411690863.py:2: DtypeWarning: Columns (8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  conf_df = pd.read_csv(\"/home/shahul/Data/one-million-reddit-confessions.csv\")\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"allenai/prosocial-dialog\",split=\"train\")\n",
    "conf_df = pd.read_csv(\"/home/shahul/Data/one-million-reddit-confessions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c86567c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_142180/162276847.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  conf_df_filtered[\"title_len\"] = conf_df_filtered['title'].map(lambda x : len(x.split()))\n"
     ]
    }
   ],
   "source": [
    "conf_df_filtered = conf_df.query(\"score>3.0\")\n",
    "conf_df_filtered[\"title_len\"] = conf_df_filtered['title'].map(lambda x : len(x.split()))\n",
    "conf_df_filtered = conf_df_filtered.query(\"title_len>5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0a3d035",
   "metadata": {},
   "outputs": [],
   "source": [
    "rots = [item[\"rots\"] for item in dataset]\n",
    "rots = set([x for item in rots for x in item])\n",
    "rots = list(rots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a169a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_vectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3580d6db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "110a6ff1520442a59e81d1c25f4d8fd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/3630 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rot_vector = vectorize_text(model, rots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "828a26ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01d5bce490134da78f788fc1d7917b1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5431 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "posts = conf_df_filtered['title'].values.tolist()\n",
    "posts_vector = vectorize_text(model,posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7bb4d884",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████| 174/174 [09:21<00:00,  3.23s/it]\n"
     ]
    }
   ],
   "source": [
    "match_dict = get_conversations(rot_vector,posts_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4f6ad39",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = convert_to_json(match_dict,rots,conf_df_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5c2d7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99dc7c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "confession_dataset = Dataset.from_list(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6face5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['context', 'rots', 'permalink', 'episone_done'],\n",
       "    num_rows: 14805\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confession_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26477b70",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Pseudo label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ceb5489c",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_id = {\"__casual__\":0,\"__needs_caution__\":1,\"__needs_intervention__\":2,\"__probably_needs_caution__\":3,\"__possibly_needs_caution__\":4}\n",
    "id_to_label = {v:k for k,v in label_to_id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2894b020",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "class ProSocialDataset(Dataset):\n",
    "    \n",
    "    def __init__(self,dataset,tokenizer):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.sep_token  = self.tokenizer.sep_token\n",
    "        self.dataset = dataset\n",
    "        self.label2id = label_to_id\n",
    "        self.id2label = {v:k for k,v in label_to_id.items()}\n",
    "        self.max_len = MAXLEN\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        encoding = {}\n",
    "\n",
    "        \n",
    "        context = self.dataset[idx][\"context\"]\n",
    "        rots = self.dataset[idx][\"rots\"]\n",
    "        input_tokens = self.tokenizer.encode(self.dataset[idx][\"context\"],add_special_tokens=False)\n",
    "        max_len = max(0,self.max_len - (len(input_tokens)+4))\n",
    "\n",
    "        rots = self.tokenizer.encode(self.tokenizer.sep_token.join(rots),\n",
    "                                     add_special_tokens=False,\n",
    "                                   max_length=max_len,)\n",
    "        \n",
    "        \n",
    "        input_ids = [0] + input_tokens + [self.tokenizer.sep_token_id] + rots + [self.tokenizer.eos_token_id]\n",
    "        input_ids = input_ids + [self.tokenizer.pad_token_id] * max(0,(self.max_len - len(input_ids)))\n",
    "        mask = [1]*len(input_ids) + [self.tokenizer.pad_token_id] * (self.max_len-len(input_ids))\n",
    "        \n",
    "        encoding[\"input_ids\"] = torch.tensor(input_ids)\n",
    "        encoding[\"attention_mask\"] = torch.tensor(mask)\n",
    "        \n",
    "        return encoding\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31588090",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"shahules786/prosocial-classifier\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d1c061fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0753f557de024b7a85512e3a24b49646",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fd1b24e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(pred_dataloader):\n",
    "    \n",
    "    probs, labels = [], []\n",
    "    for data in tqdm(pred_dataloader):\n",
    "        pred = softmax(\n",
    "            model(data[\"input_ids\"], data[\"attention_mask\"])\n",
    "            .logits.detach()\n",
    "            .numpy()\n",
    "            .astype(\"float16\"),\n",
    "            axis=1,\n",
    "        )\n",
    "        probs.append(pred.max(axis=1))\n",
    "        labels.append([model.config.id2label.get(pred_id) for pred_id in pred.argmax(axis=1)])\n",
    "\n",
    "    return np.concatenate(probs), np.concatenate(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "26699750",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dataset = ProSocialDataset(confession_dataset,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f96786ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dataset = DataLoader(pred_dataset,batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3834fae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                       | 0/3702 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "100%|████████████| 3702/3702 [48:05<00:00,  1.28it/s]\n"
     ]
    }
   ],
   "source": [
    "prob,label=predict(pred_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "73213814",
   "metadata": {},
   "outputs": [],
   "source": [
    "confession_dataset = confession_dataset.add_column(\"confidence\", np.array(prob,dtype='float32'))\n",
    "confession_dataset = confession_dataset.add_column(\"safety_label\", label)\n",
    "confession_dataset = confession_dataset.add_column(\"response\", [None]*len(confession_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "262b7ba0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3994d24273604188b5634ab994000fc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae172ed03ffc40c48a16a2f0b03f92ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/15 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a057c472be346b2b652133fd5926926",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 1 LFS files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6fad26f710643c5b040d0b87a23c551",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Deleting unused files from dataset repository:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84a8f2c0e3304ba0aa22ab019479b5ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/592 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating downloaded metadata with the new split.\n"
     ]
    }
   ],
   "source": [
    "confession_dataset.push_to_hub(\"shahules786/prosocial-confessions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8a7b01a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['context', 'rots', 'permalink', 'episone_done', 'confidence', 'safety_label', 'response'],\n",
       "    num_rows: 14805\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confession_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50a896e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OpenAssistant",
   "language": "python",
   "name": "openassistant"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
