# coding=utf-8
# Copyright 2023 The Google Research Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Training set split generation.

Specifically, we support three types of training/test splits:
- IID: random split of examples in train/test.
- OOD: reandomly splitting InferenceProblems, and then generating train/test
  examples separately from each subset of InferenceProblems.
- length: splits examples by length (defined as the number of premises in the
  InferenceProblem used to generate the example).
"""


import example_generation as eg
import inference_problems as ip
import logic_inference_lib as lib


def generate_training_and_test_sets_iid(n_problems, n_variations, n_examples,
                                        train_ratio,
                                        example_types=None,
                                        length_distribution=None,
                                        answer_at_the_end=True):
  """Generates an IID split."""

  problems = ip.generate_multistep_problems(
      n_problems, length_distribution=length_distribution)
  actual_length_distribution = ip.problem_length_stats(problems)
  print(f"problem counts by chain length: {actual_length_distribution}")
  examples = eg.generate_examples_from_problems(
      problems, n_variations, n_examples,
      example_types=example_types,
      answer_at_the_end=answer_at_the_end)
  split_point = int(train_ratio * len(examples))
  examples_train = examples[:split_point]
  examples_test = examples[split_point:]
  return examples_train, examples_test


def generate_training_and_test_sets_ood(n_problems, n_variations, n_examples,
                                        train_ratio,
                                        example_types=None,
                                        length_distribution=None,
                                        answer_at_the_end=True):
  """Generates an OOD split."""

  def ensure_all_rules_appear_in_training(problems_train, problems_test):
    counts_train = ip.rules_used_in_problems(problems_train)
    counts_test = ip.rules_used_in_problems(problems_test)
    missing_rules = []
    for rule_name in lib.ALL_RULE_NAMES:
      if rule_name not in counts_train:
        counts_train[rule_name] = 0
      if rule_name not in counts_test:
        counts_test[rule_name] = 0
      if counts_test[rule_name] > 0:
        if counts_train[rule_name] <= 0:
          if rule_name not in missing_rules:
            missing_rules.append(rule_name)
    if not missing_rules:
      # We are fine!
      return
    print(f"missing: {missing_rules}")
    for rule_name in lib.ALL_RULE_NAMES:
      print(f"{rule_name}\t{counts_train[rule_name]}\t{counts_test[rule_name]}")
    raise ValueError("Some rule types appear in testing, but not in training!")

  problems = ip.generate_multistep_problems(
      n_problems, length_distribution=length_distribution)
  actual_length_distribution = ip.problem_length_stats(problems)
  print(f"problem counts by chain length: {actual_length_distribution}")
  # For the OOD setting, we split by the problem type early on, so that testing
  # has constructs, not seen during training:
  problems_split_point = int(train_ratio * len(problems))
  problems_train = problems[:problems_split_point]
  problems_test = problems[problems_split_point:]

  ensure_all_rules_appear_in_training(problems_train, problems_test)

  examples_train = eg.generate_examples_from_problems(
      problems_train, n_variations, int(n_examples*train_ratio),
      example_types=example_types,
      answer_at_the_end=answer_at_the_end)
  examples_test = eg.generate_examples_from_problems(
      problems_test, n_variations, int(n_examples*(1-train_ratio)),
      example_types=example_types,
      answer_at_the_end=answer_at_the_end)

  return examples_train, examples_test


def generate_training_and_test_sets_length(n_problems, n_variations, n_examples,
                                           length_threshold,
                                           example_types=None,
                                           length_distribution=None,
                                           answer_at_the_end=True):
  """Generates a length split."""

  problems = ip.generate_multistep_problems(
      n_problems, length_distribution=length_distribution)
  actual_length_distribution = ip.problem_length_stats(problems)
  print(f"problem counts by chain length: {actual_length_distribution}")
  examples = eg.generate_examples_from_problems(
      problems, n_variations, n_examples,
      example_types=example_types,
      answer_at_the_end=answer_at_the_end)
  examples_train = []
  examples_test = []
  for example in examples:
    if len(example.problem.premises) <= length_threshold:
      examples_train.append(example)
    else:
      examples_test.append(example)
  return examples_train, examples_test
