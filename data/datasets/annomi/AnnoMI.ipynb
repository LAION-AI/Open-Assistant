{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98fecbf5",
   "metadata": {},
   "source": [
    "### Instructions to process the multi-turn dialog dataset of \n",
    "\n",
    "Aknowledgements to: Wu, Zixiu and Balloccu, Simone and Kumar, Vivek and Helaoui, Rim and Reiter, Ehud and Reforgiato Recupero, Diego and Riboni, Daniele\n",
    "\n",
    "This data first appeared in the paper: </br> \n",
    "Anno-MI: A Dataset of Expert-Annotated Counselling Dialogues in ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). </br>\n",
    "URL: https://ieeexplore.ieee.org/document/9746035/;jsessionid=W0Qfv89lx5LjTNei3Khmb2sjFcebu3IPajU2spOhFNBob0rhWZb7!1808242117"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3d664d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from functools import reduce\n",
    "from typing import Dict\n",
    "\n",
    "# download the data from github\n",
    "df = pd.read_csv(\"https://github.com/uccollab/AnnoMI/blob/main/AnnoMI-full.csv?raw=true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "08ab0119",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_node(text: str, role: str) -> Dict:\n",
    "    return {\"text\": text, \"role\": role, \"meta\": {}, \"replies\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c17b3e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "threads = []\n",
    "\n",
    "for transcript_id in df[\"transcript_id\"].unique():\n",
    "    transcript = df[df[\"transcript_id\"] == transcript_id]\n",
    "    # since we are using a reduce strategy, we must sort in the reverse order\n",
    "    transcript = transcript.sort_values(\"timestamp\", ascending=False)\n",
    "    transcript_nodes = transcript.apply(lambda x: transform_to_node(x[\"utterance_text\"], x[\"interlocutor\"]), axis=1)\n",
    "    thread = {\n",
    "        \"thread\": reduce(\n",
    "            lambda x, y: {\"text\": y[\"text\"], \"role\": y[\"role\"], \"meta\": {}, \"replies\": [x]},  # therapist or client\n",
    "            transcript_nodes.values,\n",
    "        ),\n",
    "        \"source\": f'Transcript of {transcript[\"video_url\"].iloc[0]} extracted from https://www.youtube.com/watch?v=PaSKcfTmFEk',\n",
    "        \"meta\": {},\n",
    "    }\n",
    "    threads.append(thread)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openassistant-data",
   "language": "python",
   "name": "openassistant-data"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
