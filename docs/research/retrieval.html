<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-research/retrieval">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.4.0">
<title data-rh="true">Retrieval Directions and Research Papers | Open Assistant</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://LAION-AI.github.io/Open-Assistant/docs/research/retrieval"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Retrieval Directions and Research Papers | Open Assistant"><meta data-rh="true" name="description" content="Dataset and Benchmark"><meta data-rh="true" property="og:description" content="Dataset and Benchmark"><link data-rh="true" rel="icon" href="/Open-Assistant/img/logo.svg"><link data-rh="true" rel="canonical" href="https://LAION-AI.github.io/Open-Assistant/docs/research/retrieval"><link data-rh="true" rel="alternate" href="https://LAION-AI.github.io/Open-Assistant/docs/research/retrieval" hreflang="en"><link data-rh="true" rel="alternate" href="https://LAION-AI.github.io/Open-Assistant/docs/research/retrieval" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/Open-Assistant/blog/rss.xml" title="OpenAssistant Blog RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/Open-Assistant/blog/atom.xml" title="OpenAssistant Blog Atom Feed">
<link rel="alternate" type="application/json" href="/Open-Assistant/blog/feed.json" title="OpenAssistant Blog JSON Feed"><link rel="stylesheet" href="/Open-Assistant/assets/css/styles.83321857.css">
<link rel="preload" href="/Open-Assistant/assets/js/runtime~main.2c689d1f.js" as="script">
<link rel="preload" href="/Open-Assistant/assets/js/main.6f0a1453.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}return t}()||function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/Open-Assistant/"><div class="navbar__logo"><img src="/Open-Assistant/img/logo.svg" alt="Open Assistant Logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/Open-Assistant/img/logo.svg" alt="Open Assistant Logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">Open Assistant</b></a><a href="https://open-assistant.io/" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">App<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><a href="https://open-assistant.io/chat" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">Chat<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><a class="navbar__item navbar__link" href="/Open-Assistant/docs/intro">Docs</a><a class="navbar__item navbar__link" href="/Open-Assistant/blog">Blog</a><a class="navbar__item navbar__link" href="/Open-Assistant/api">API</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/LAION-AI/Open-Assistant" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0 docsWrapper_BCFX"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docPage__5DB"><main class="docMainContainer_gTbr docMainContainerEnhanced_Uz_u"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><h1>Retrieval Directions and Research Papers</h1><h2 class="anchor anchorWithStickyNavbar_LWe7" id="dataset-and-benchmark">Dataset and Benchmark<a href="#dataset-and-benchmark" class="hash-link" aria-label="Direct link to Dataset and Benchmark" title="Direct link to Dataset and Benchmark">​</a></h2><ul><li>BEIR
<a href="https://arxiv.org/abs/2104.08663v4" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/2104.08663v4</a> -
Benchmark for Information Retrieval</li><li>MS MARCO(part of BEIR)
<a href="https://arxiv.org/abs/1611.09268v3" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/1611.09268v3</a> -
Machine Reading Comprehension Dataset / Retrieval Benchmark</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="search-algorithm">Search Algorithm<a href="#search-algorithm" class="hash-link" aria-label="Direct link to Search Algorithm" title="Direct link to Search Algorithm">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="links">Links<a href="#links" class="hash-link" aria-label="Direct link to Links" title="Direct link to Links">​</a></h3><ul><li>ElasticSearch:
<a href="https://www.elastic.co/elasticsearch" target="_blank" rel="noopener noreferrer">https://www.elastic.co/elasticsearch</a></li><li>Apache Lucene: <a href="https://lucene.apache.org/" target="_blank" rel="noopener noreferrer">https://lucene.apache.org/</a></li><li>Meta Faiss:
<a href="https://github.com/facebookresearch/faiss" target="_blank" rel="noopener noreferrer">https://github.com/facebookresearch/faiss</a></li><li>Google Scann:
<a href="https://github.com/google-research/google-research/tree/master/scann" target="_blank" rel="noopener noreferrer">https://github.com/google-research/google-research/tree/master/scann</a></li><li>Qdrant Vector DB:
<a href="https://github.com/qdrant/qdrant" target="_blank" rel="noopener noreferrer">https://github.com/qdrant/qdrant</a></li><li>Milvus Vector DB: <a href="https://milvus.io/" target="_blank" rel="noopener noreferrer">https://milvus.io/</a></li><li>Open Retrieval Index Code:
<a href="https://github.com/kenhktsui/open-information-retrieval" target="_blank" rel="noopener noreferrer">https://github.com/kenhktsui/open-information-retrieval</a></li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="relevant-papers">Relevant Papers<a href="#relevant-papers" class="hash-link" aria-label="Direct link to Relevant Papers" title="Direct link to Relevant Papers">​</a></h3><ul><li>FAISS: <a href="https://arxiv.org/abs/1702.08734" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/1702.08734</a> -
vector index by Facebook</li><li>SCaNN: <a href="https://arxiv.org/abs/1908.10396" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/1908.10396</a> -
vector index by Google</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="1-retrieval-index">1. Retrieval-Index<a href="#1-retrieval-index" class="hash-link" aria-label="Direct link to 1. Retrieval-Index" title="Direct link to 1. Retrieval-Index">​</a></h2><p>At first, either a rule-based search or sparse vector search (e.g. BM25) or a
dense vector search (semantic search) (e.g. BERT, Contriever) could be used. In
practice, retrieval is a layered approach, where the first search is optimised
for recall and reranking is optimised for precision.</p><p>The first search in general is a sparse vector search, or dense vector search
(bi-encoder). The advantage is that it is fast because document can be
pre-indexed and stored in a DB. Cosine similarity is used to find the most
similar pre-indexed document embedding given the query embedding. Reranking is a
technique to boost the performance of top K documents from the first search. For
example, cross-encoder which takes both query and document into a language
model, and output a scalar relevance between 0 and 1. It has more superior
performance than bi-encoder because it allows interaction of query and document
in the language model. But it is slow because no index can be pre-computed.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="links-1">Links<a href="#links-1" class="hash-link" aria-label="Direct link to Links" title="Direct link to Links">​</a></h3><ul><li>LangChain:
<a href="https://github.com/hwchase17/langchain" target="_blank" rel="noopener noreferrer">https://github.com/hwchase17/langchain</a> -
Plugins around any language model</li><li>LlamaIndex:
<a href="https://github.com/jerryjliu/llama_index" target="_blank" rel="noopener noreferrer">https://github.com/jerryjliu/llama_index</a> -
General Retrieval System for LMs and external data</li><li>LlamaHub: <a href="https://llamahub.ai/" target="_blank" rel="noopener noreferrer">https://llamahub.ai/</a> - Data Source Plugins
for LlamaIndex</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="relevant-papers-1">Relevant Papers<a href="#relevant-papers-1" class="hash-link" aria-label="Direct link to Relevant Papers" title="Direct link to Relevant Papers">​</a></h3><ul><li>SBERT <a href="https://arxiv.org/abs/1908.10084" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/1908.10084</a></li><li>BM25+CE
<a href="https://arxiv.org/abs/2104.08663v4" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/2104.08663v4</a></li><li>RALM <a href="https://arxiv.org/abs/2302.00083" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/2302.00083</a></li><li>ColBert <a href="https://arxiv.org/abs/2004.12832" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/2004.12832</a></li><li>DPR <a href="https://arxiv.org/abs/2004.04906" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/2004.04906</a></li><li>UPR <a href="https://arxiv.org/abs/2204.07496" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/2204.07496</a></li><li>...</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="2-plugin-based-approach">2. Plugin-based approach<a href="#2-plugin-based-approach" class="hash-link" aria-label="Direct link to 2. Plugin-based approach" title="Direct link to 2. Plugin-based approach">​</a></h2><p>In this approach, retrieval as a tool, is embedded into the training data,
including:</p><ul><li>when a retrieval is required</li><li>how to do a search (what to search)</li><li>how to use search result As such, a language model trained with this data is
able to do retrieval from a next token prediction objective.</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="relevant-papers-2">Relevant Papers<a href="#relevant-papers-2" class="hash-link" aria-label="Direct link to Relevant Papers" title="Direct link to Relevant Papers">​</a></h3><ul><li>Toolformer: <a href="http://arxiv.org/abs/2302.04761" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2302.04761</a></li><li>...</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="3-embedding-based-approach">3. Embedding-based approach<a href="#3-embedding-based-approach" class="hash-link" aria-label="Direct link to 3. Embedding-based approach" title="Direct link to 3. Embedding-based approach">​</a></h2><p>The embedding-based approach ingests retrieved information directly into the
model, e.g. via an additional encoder and cross-attention.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="3a">3a<a href="#3a" class="hash-link" aria-label="Direct link to 3a" title="Direct link to 3a">​</a></h3><p>Simply inject embeddings via cross-attention or a similar mechanism.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="3b">3b<a href="#3b" class="hash-link" aria-label="Direct link to 3b" title="Direct link to 3b">​</a></h3><p>Inject embeddings based on a more sophisticated architecture, e.g. make the
model decide to do retrieval and only then inject embeddings. Might be hard to
train.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="3c">3c<a href="#3c" class="hash-link" aria-label="Direct link to 3c" title="Direct link to 3c">​</a></h3><p>Train retrieval index jointly with the injection. Possibly infeasible as the
index needs to be re-updated during training.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="relevant-papers-3">Relevant papers<a href="#relevant-papers-3" class="hash-link" aria-label="Direct link to Relevant papers" title="Direct link to Relevant papers">​</a></h3><ul><li>RETRO: <a href="http://arxiv.org/abs/2112.04426" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2112.04426</a></li><li>REALM: <a href="https://arxiv.org/abs/2002.08909" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/2002.08909</a></li><li>RAG: <a href="https://arxiv.org/abs/2005.11401" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/2005.11401</a></li><li>Atlas <a href="https://arxiv.org/abs/2208.03299" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/2208.03299</a></li><li>Unilimiformer
<a href="http://arxiv.org/abs/2305.01625" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2305.01625</a></li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="paper-summaries">Paper summaries<a href="#paper-summaries" class="hash-link" aria-label="Direct link to Paper summaries" title="Direct link to Paper summaries">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="borgeaud-et-al-2020-improving-language-models-by-retrieving-from-trillions-of-tokens---retro">Borgeaud et al. 2020: Improving Language Models by Retrieving from Trillions of Tokens - &quot;RETRO&quot;<a href="#borgeaud-et-al-2020-improving-language-models-by-retrieving-from-trillions-of-tokens---retro" class="hash-link" aria-label="Direct link to Borgeaud et al. 2020: Improving Language Models by Retrieving from Trillions of Tokens - &quot;RETRO&quot;" title="Direct link to Borgeaud et al. 2020: Improving Language Models by Retrieving from Trillions of Tokens - &quot;RETRO&quot;">​</a></h3><p>Idea: Use BERT (Devlin et al. 2018) as a contextual encoder for chunks of size
64 of the training data. Then train an encoder-decoder transformer model with
inputs and similar (not too similar / same) input chunks retrieved by BERT
embedding similarity - all done in a causal way (retrieve only &quot;from the past&quot;).
The Cross-Attention is replaced by a Chunked Cross Attention optimized for
batches of similar retrieved chunks. They pre-filter their dataset such that
data duplicates cannot easily leak information via retrieval. This was scaled to
2T tokens and a 7.5 B parameter model exceeding GPT-3 performance. RETROfitting
of a pre-trained transformer also works, with small losses in perplexity (0.3),
but a lot faster training (6 % of training sequences = 6M seq à 2048 tokens).
This is not fine-tuning but just training the cross-attention, keeping
pre-trained weights fixed. Larger models benefit from more nearest neighbors,
i.e. the 7B can utilize 40 nearest neighbor chunks, a 172M model only 10 NNs.</p><p><a href="http://arxiv.org/abs/2112.04426" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2112.04426</a></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="bertsch-et-al-2023-unlimiformer-long-range-transformers-with-unlimited-length-input">Bertsch et al. 2023: Unlimiformer: Long-Range Transformers with Unlimited Length Input<a href="#bertsch-et-al-2023-unlimiformer-long-range-transformers-with-unlimited-length-input" class="hash-link" aria-label="Direct link to Bertsch et al. 2023: Unlimiformer: Long-Range Transformers with Unlimited Length Input" title="Direct link to Bertsch et al. 2023: Unlimiformer: Long-Range Transformers with Unlimited Length Input">​</a></h3><p>Idea: Use retrieval to actually maximize overlap of &quot;query embeddings&quot; with
embeddings from an encoder (in a encoder-decoder architecture). Essentially it
is an ideal approximation of the softmax in the Cross-Attention over all
previous tokens (in the encoder inputs).</p><p>Code:
<a href="https://github.com/abertsch72/unlimiformer" target="_blank" rel="noopener noreferrer">https://github.com/abertsch72/unlimiformer</a>
Paper: <a href="http://arxiv.org/abs/2305.01625" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2305.01625</a></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="izacard-et-al-2022-unsupervised-dense-information-retrieval-with-contrastive-learning---contriver">Izacard et al. 2022: Unsupervised Dense Information Retrieval with Contrastive Learning - &quot;Contriver&quot;<a href="#izacard-et-al-2022-unsupervised-dense-information-retrieval-with-contrastive-learning---contriver" class="hash-link" aria-label="Direct link to Izacard et al. 2022: Unsupervised Dense Information Retrieval with Contrastive Learning - &quot;Contriver&quot;" title="Direct link to Izacard et al. 2022: Unsupervised Dense Information Retrieval with Contrastive Learning - &quot;Contriver&quot;">​</a></h3><p>They present Contriver, an open-source implementation of their novel approach to
information retrieval using neural networks that outperforms traditional methods
and can be applied to a wide range of retrieval settings. The main idea behind
Contriver is to use contrastive learning to train dense retrievers for
information retrieval. Their key contribution is showing that this approach
leads to strong performance in various retrieval settings, including
cross-lingual retrieval, and outperforms traditional unsupervised term-frequency
methods such as BM25.</p><p>Specifically, on the BEIR benchmark, their unsupervised model outperforms BM25
on 11 out of 15 datasets for the Recall@100. When used as pre-training before
fine-tuning, either on a few thousands in-domain examples or on the large MS
MARCO dataset, their contrastive model leads to improvements on the BEIR
benchmark.</p><p>Pre-trained model and source code are available on Huggingface and GitHub.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="schick-et-al-2023-toolformer-language-models-can-teach-themselves-to-use-tools">Schick et al. 2023: Toolformer: Language Models Can Teach Themselves to Use Tools<a href="#schick-et-al-2023-toolformer-language-models-can-teach-themselves-to-use-tools" class="hash-link" aria-label="Direct link to Schick et al. 2023: Toolformer: Language Models Can Teach Themselves to Use Tools" title="Direct link to Schick et al. 2023: Toolformer: Language Models Can Teach Themselves to Use Tools">​</a></h3><p>They use in-context learning of GPT-3 and some handcrafted samples to annotate a
language modeling dataset with potential uses of external tools, like QA,
wikipedia search, a calculator, machine translation and a calendar - via text
tags for those tools and respective tool queries. They use this data then to
fine-tune GPT-2/GPT-J models, implement according tools and train with up to 25k
examples per API, max sequence length 1,024. They outperform other language
models with large margin when using tools and are comparable to larger ones when
only fine-tuned on the tool-based dataset.</p><p><a href="http://arxiv.org/abs/2302.04761" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2302.04761</a></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="guu-et-al-2020-realm-retrieval-augmented-language-model-pre-training">Guu et al 2020: REALM: Retrieval-Augmented Language Model Pre-Training<a href="#guu-et-al-2020-realm-retrieval-augmented-language-model-pre-training" class="hash-link" aria-label="Direct link to Guu et al 2020: REALM: Retrieval-Augmented Language Model Pre-Training" title="Direct link to Guu et al 2020: REALM: Retrieval-Augmented Language Model Pre-Training">​</a></h3><p>They use retrieved information from a KB to train a MLM self-supervised and
evaluate on QA tasks. Predecessor to RETRO.</p><p>The authors of the paper structure the retriever in REALM such that the
computation performed for each document can be cached and asynchronously
updated, and selection of the best documents can be formulated as Maximum Inner
Product Search (MIPS). This allows for efficient retrieval of potentially
relevant documents from a large corpus during pre-training.</p><p>During pre-training, REALM backpropagates through the retrieval step that
considers millions of documents, but it does not backpropagate to each
individual document. Instead, it uses a single encoder to encode the subset of
retrieved samples and then backpropagates through this encoder. This approach
allows for efficient computation during pre-training while still allowing for
effective utilization of world knowledge.</p><p>(<a href="https://arxiv.org/abs/2002.08909)%5Bhttps://arxiv.org/abs/2002.08909%5D" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/2002.08909)[https://arxiv.org/abs/2002.08909]</a></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="zamani-et-al-2022-retrieval-enhanced-machine-learning">Zamani et al. 2022: Retrieval-Enhanced Machine Learning<a href="#zamani-et-al-2022-retrieval-enhanced-machine-learning" class="hash-link" aria-label="Direct link to Zamani et al. 2022: Retrieval-Enhanced Machine Learning" title="Direct link to Zamani et al. 2022: Retrieval-Enhanced Machine Learning">​</a></h3><p>This paper introduces a new research program called Retrieval-Enhanced Machine
Learning (REML), which combines information retrieval techniques with machine
learning to improve model accuracy and interpretability. The authors describe
the core principles of indexing, representation, retrieval, and ranking that
underlie REML models, and provide examples of how these models have been applied
in real-world scenarios.</p><p>The main contribution of this paper is to lay out a research agenda for REML
that includes several key challenges and opportunities for future work. These
include developing new optimization algorithms that can handle large-scale data
sets, exploring the use of deep learning architectures in conjunction with
retrieval-based methods, and investigating the impact of different retrieval
strategies on model performance.</p><p>Overall, the key idea behind REML is to leverage the strengths of both
information retrieval and machine learning to create more powerful and flexible
models that can handle complex data sets and produce more accurate results. By
combining these two fields, researchers hope to pave the way for new advances in
artificial intelligence and information access research.</p><p>(<a href="https://arxiv.org/abs/2205.01230)%5Bhttps://arxiv.org/abs/2205.01230%5D" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/2205.01230)[https://arxiv.org/abs/2205.01230]</a></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="thakur-et-al-2021-beir-a-heterogeneous-benchmark-for-zero-shot-evaluation-of-information-retrieval-models">Thakur et al. 2021: BEIR: A Heterogeneous Benchmark for Zero-shot Evaluation of Information Retrieval Models<a href="#thakur-et-al-2021-beir-a-heterogeneous-benchmark-for-zero-shot-evaluation-of-information-retrieval-models" class="hash-link" aria-label="Direct link to Thakur et al. 2021: BEIR: A Heterogeneous Benchmark for Zero-shot Evaluation of Information Retrieval Models" title="Direct link to Thakur et al. 2021: BEIR: A Heterogeneous Benchmark for Zero-shot Evaluation of Information Retrieval Models">​</a></h3><p>The BEIR benchmarking tool is designed to provide a comprehensive evaluation of
information retrieval models across diverse tasks and domains. It includes 18
retrieval datasets for comparison and evaluation of model generalization,
spanning nine different retrieval tasks such as fact checking, citation
prediction, duplicate question retrieval, argument retrieval, news retrieval,
question answering, tweet retrieval, bio-medical IR, and entity retrieval. The
selection methodology is motivated by the need for diverse tasks and domains to
evaluate the zero-shot capabilities of retrieval systems. The tool is
open-sourced with a standardized data format and easy-to-adapt code examples for
many different retrieval strategies.</p><p>They compare neural retrieval to legacy systems like BM25 and show that BM25 is
still a very strong baseline. The best model is a BM25 based search with
additional re-ranking based on a neural classifier.</p><p>Observations:</p><ol><li>&quot;In-domain performance is not a good indicator for out-of-domain
generalization&quot;</li><li>&quot;Term-weighting fails, document expansion captures out-of-domain keyword
vocabulary&quot;</li><li>&quot;Dense retrieval models with issues for out-of-distribution data&quot;</li><li>&quot;Re-ranking and Late-Interaction models generalize well to
out-of-distribution data&quot;</li><li>&quot;Strong training losses for dense retrieval leads to better
out-of-distribution performances&quot;</li><li>&quot;TAS-B model prefers to retrieve documents with shorter lengths&quot;</li></ol><p>Conclusion: Maybe not only focus on a vector-based index, use a standard index
as base + neural re-ranking</p><p>(<a href="https://arxiv.org/pdf/2104.08663.pdf)%5Bhttps://arxiv.org/pdf/2104.08663.pdf%5D" target="_blank" rel="noopener noreferrer">https://arxiv.org/pdf/2104.08663.pdf)[https://arxiv.org/pdf/2104.08663.pdf]</a></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="other-interesting-papers">Other interesting papers<a href="#other-interesting-papers" class="hash-link" aria-label="Direct link to Other interesting papers" title="Direct link to Other interesting papers">​</a></h2><ul><li><p>Nakano et al: WebGPT (predecessor to ChatGPT) - fine-tune GPT3 to search the
web for QA tasks
(<a href="https://arxiv.org/pdf/2112.09332.pdf)%5Bhttps://arxiv.org/pdf/2112.09332.pdf%5D" target="_blank" rel="noopener noreferrer">https://arxiv.org/pdf/2112.09332.pdf)[https://arxiv.org/pdf/2112.09332.pdf]</a></p></li><li><p>Schick et al: PEER: A Collaborative Language Model
(<a href="https://arxiv.org/pdf/2208.11663.pdf)%5Bhttps://arxiv.org/pdf/2208.11663.pdf%5D" target="_blank" rel="noopener noreferrer">https://arxiv.org/pdf/2208.11663.pdf)[https://arxiv.org/pdf/2208.11663.pdf]</a></p></li><li><p>Goyal et al. 2023: Retrieval Augmented Reinforcement Learning</p></li><li><p>Humphreys et al. 2022: Large-Scale Retrieval for Reinforcement Learning</p></li></ul></div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages navigation"></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#dataset-and-benchmark" class="table-of-contents__link toc-highlight">Dataset and Benchmark</a></li><li><a href="#search-algorithm" class="table-of-contents__link toc-highlight">Search Algorithm</a><ul><li><a href="#links" class="table-of-contents__link toc-highlight">Links</a></li><li><a href="#relevant-papers" class="table-of-contents__link toc-highlight">Relevant Papers</a></li></ul></li><li><a href="#1-retrieval-index" class="table-of-contents__link toc-highlight">1. Retrieval-Index</a><ul><li><a href="#links-1" class="table-of-contents__link toc-highlight">Links</a></li><li><a href="#relevant-papers-1" class="table-of-contents__link toc-highlight">Relevant Papers</a></li></ul></li><li><a href="#2-plugin-based-approach" class="table-of-contents__link toc-highlight">2. Plugin-based approach</a><ul><li><a href="#relevant-papers-2" class="table-of-contents__link toc-highlight">Relevant Papers</a></li></ul></li><li><a href="#3-embedding-based-approach" class="table-of-contents__link toc-highlight">3. Embedding-based approach</a><ul><li><a href="#3a" class="table-of-contents__link toc-highlight">3a</a></li><li><a href="#3b" class="table-of-contents__link toc-highlight">3b</a></li><li><a href="#3c" class="table-of-contents__link toc-highlight">3c</a></li><li><a href="#relevant-papers-3" class="table-of-contents__link toc-highlight">Relevant papers</a></li></ul></li><li><a href="#paper-summaries" class="table-of-contents__link toc-highlight">Paper summaries</a><ul><li><a href="#borgeaud-et-al-2020-improving-language-models-by-retrieving-from-trillions-of-tokens---retro" class="table-of-contents__link toc-highlight">Borgeaud et al. 2020: Improving Language Models by Retrieving from Trillions of Tokens - &quot;RETRO&quot;</a></li><li><a href="#bertsch-et-al-2023-unlimiformer-long-range-transformers-with-unlimited-length-input" class="table-of-contents__link toc-highlight">Bertsch et al. 2023: Unlimiformer: Long-Range Transformers with Unlimited Length Input</a></li><li><a href="#izacard-et-al-2022-unsupervised-dense-information-retrieval-with-contrastive-learning---contriver" class="table-of-contents__link toc-highlight">Izacard et al. 2022: Unsupervised Dense Information Retrieval with Contrastive Learning - &quot;Contriver&quot;</a></li><li><a href="#schick-et-al-2023-toolformer-language-models-can-teach-themselves-to-use-tools" class="table-of-contents__link toc-highlight">Schick et al. 2023: Toolformer: Language Models Can Teach Themselves to Use Tools</a></li><li><a href="#guu-et-al-2020-realm-retrieval-augmented-language-model-pre-training" class="table-of-contents__link toc-highlight">Guu et al 2020: REALM: Retrieval-Augmented Language Model Pre-Training</a></li><li><a href="#zamani-et-al-2022-retrieval-enhanced-machine-learning" class="table-of-contents__link toc-highlight">Zamani et al. 2022: Retrieval-Enhanced Machine Learning</a></li><li><a href="#thakur-et-al-2021-beir-a-heterogeneous-benchmark-for-zero-shot-evaluation-of-information-retrieval-models" class="table-of-contents__link toc-highlight">Thakur et al. 2021: BEIR: A Heterogeneous Benchmark for Zero-shot Evaluation of Information Retrieval Models</a></li></ul></li><li><a href="#other-interesting-papers" class="table-of-contents__link toc-highlight">Other interesting papers</a></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://ykilcher.com/open-assistant-discord" target="_blank" rel="noopener noreferrer" class="footer__link-item">OpenAssistant Contributors Discord<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://discord.com/invite/mVcgxMPD7e" target="_blank" rel="noopener noreferrer" class="footer__link-item">LAION Discord<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://ykilcher.com/discord" target="_blank" rel="noopener noreferrer" class="footer__link-item">YK Discord<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">Resources</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/LAION-AI/Open-Assistant" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://projects.laion.ai/Open-Assistant/docs/faq" target="_blank" rel="noopener noreferrer" class="footer__link-item">FAQ<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2023 laion.ai. Built with Docusaurus.</div></div></div></footer></div>
<script src="/Open-Assistant/assets/js/runtime~main.2c689d1f.js"></script>
<script src="/Open-Assistant/assets/js/main.6f0a1453.js"></script>
</body>
</html>