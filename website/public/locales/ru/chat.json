{
  "back_to_chat_list": "Вернуться к диалогам",
  "chat_date": "{{val, datetime}}",
  "config_title": "Настройки диалога",
  "empty": "(без названия)",
  "login_message": "Чтобы воспользоваться диалогами, вам нужно повторно войти. Используйте один из способов ниже:",
  "max_new_tokens": "Максимальное кол-во новых токенов",
  "model": "Модель",
  "preset": "Предустановка",
  "parameter_description": {
    "max_new_tokens": "Максимальное количество новых токенов: Этот параметр указывает модели, сколько максимум новых токенов она должна сгенерировать для ответа.",
    "repetition_penalty": "Штраф за повторение: Этот параметр уменьшает вероятность повторения одних и тех же токенов снова и снова, делая повторение токенов менее вероятным, чем обычно.",
    "temperature": "Температура: Каждый генерируемый вами токен выбирается из распределения p(next_token|previous_tokens). Параметр температуры может \"обострить\" или ослабить это распределение. Установка значения 1 означает, что модель генерирует токены на основе их предсказанной вероятности (т.е. если модель предсказывает, что \"XYZ\" имеет вероятность 12.3%, она будет генерировать его с вероятностью 12.3%). Понижение температуры до нуля делает модель более жадной, заставляя высокие вероятности становиться еще выше, а низкие — еще ниже (обратите внимание, что это не линейная зависимость!). Повышение температуры делает все вероятности более схожими. Интуитивно понятно, что низкая температура означает, что модель генерирует ответы, соответствующие ее убеждениям, в то время как высокая температура позволяет получить более творческие и разнообразные ответы.",
    "top_k": "Top-k: Это похоже на выборку top-p, но вместо того, чтобы брать верхние токены, пока их суммарная вероятность не превысит 'p', берется только K наиболее вероятных токенов. Top-p обычно предпочтительнее, поскольку позволяет модели \"настраивать\" радиус поиска, но top-k может быть полезен в качестве аварийной ситуации, когда модель не знает, что генерировать дальше, и присваивает многим лексемам очень равномерное распределение.",
    "top_p": "Выборка по принципу top-p (также известная как nucleus): Этот метод уменьшает распределение вероятности и рассматривает только верхний p-процент токенов. Отбрасывая токены с низкой вероятностью, он помогает ограничить генерацию и предотвратить генерацию моделью грамматически неправильных предложений.",
    "typical_p": "Typical p: Типичная выборка — это информационно-теоретическая техника, которая, в дополнение к вероятности, учитывает энтропию последовательности (т.е. информационное содержание в соответствии с вероятностью). Это означает, что типичная выборка \"перевешивает\" некоторые токены с более низкой вероятностью, потому что они считаются \"интересными,\" и недовешивает токены с высокой вероятностью, потому что они считаются \"скучными.\""
  },
  "preset_custom": "Собственная предустановка",
  "queue_info": "Ваше сообщение находится в очереди, вы на позиции {{ queuePosition, number, integer }}.",
  "repetition_penalty": "Штраф за повтор",
  "temperature": "Температура",
  "top_k": "Top K",
  "top_p": "Top P",
  "typical_p": "Typical P",
  "you_are_logged_in": "Вы вошли в систему диалогов",
  "your_chats": "Ваши диалоги",
  "input_placeholder": "Спросите Ассистента о чём угодно",
  "warning": "Open Assistant находится на ранних этапах разработки и на данный момент не имеет доступа в Интернет. Он может генерировать неверную или вводящую в заблуждение информацию. Он не подходит для решения важных задач или для выдачи рекомендаций."
}
