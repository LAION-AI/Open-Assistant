{
  "add_plugin": "Plugin hinzufügen",
  "back_to_chat_list": "Zurück zur Chatliste",
  "chat_date": "{{val, datetime}}",
  "config_title": "Chatkonfiguration",
  "delete_chat": "Chat löschen",
  "delete_confirmation": "Sind Sie sicher, dass Sie den Chat löschen wollen?",
  "delete_confirmation_detail": "Wenn Sie diesen Chat löschen, ist er nicht mehr Teil unserer Daten und wir können ihn nicht mehr zur Verbesserung unserer Modelle verwenden. Bitte nehmen Sie sich die Zeit, Antworten in anderen Chats hoch- und runterzustufen, um uns zu helfen, Open Assistant zu verbessern!",
  "edit_plugin": "Plugin bearbeiten",
  "empty": "(leer)",
  "input_placeholder": "Fragen Sie, was Sie wollen...",
  "login_message": "Um dieses Feature zu nutzen müssen Sie sich erneut anmelden. Melden Sie sich durch einen dieser Anbieter an:",
  "max_new_tokens": "Maximale Anzahl neuer Token",
  "model": "Modell",
  "more_actions": "Weitere Aktionen",
  "only_visible": "Nur sichtbar",
  "opt_out": {
    "button": "Keine Trainingsdaten zulassen",
    "success_message": "Sie haben sich gegen die Verwendung von Trainingsdaten entschieden.",
    "dialog": {
      "title": "Trainingsdaten nicht mehr senden."
    }
  },
  "parameter_description": {
    "max_new_tokens": "Max new tokens: Dieser Parameter teilt dem Modell mit, wie viele neue Token es maximal für die Antwort generieren soll.",
    "repetition_penalty": "Wiederholungsstrafe: Dieser Parameter verringert die Wahrscheinlichkeit, dass dieselben Token immer wieder wiederholt werden, indem er wiederholte Token weniger wahrscheinlich macht, als das Modell normalerweise vorhersagen würde.",
    "temperature": "Temperatur: Jedes von Ihnen generierte Token wird aus einer Verteilung p(nächstes_Token|vorheriges_Tokens) gezogen. Der Temperaturparameter kann diese Verteilung „schärfen“ oder dämpfen. Das Festlegen auf 1 bedeutet, dass das Modell Token basierend auf ihrer vorhergesagten Wahrscheinlichkeit generiert (d. h. wenn das Modell vorhersagt, dass „XYZ“ eine Wahrscheinlichkeit von 12,3 % hat, wird es es mit einer Wahrscheinlichkeit von 12,3 % generieren). Das Absenken der Temperatur in Richtung Null macht das Modell gieriger, was dazu führt, dass hohe Wahrscheinlichkeiten noch höher und niedrige Wahrscheinlichkeiten noch niedriger werden (beachten Sie, dass dies keine lineare Beziehung ist!). Eine Erhöhung der Temperatur macht alle Wahrscheinlichkeiten ähnlicher. Intuitiv bedeutet eine niedrige Temperatur, dass das Modell Antworten generiert, die eng mit seinen Überzeugungen übereinstimmen, während eine hohe Temperatur kreativere und vielfältigere Antworten ermöglicht.",
    "top_k": "Top-k: Dies ähnelt dem Top-p-Sampling, aber anstatt die Top-Token zu nehmen, bis ihre kumulative Wahrscheinlichkeit 'p' überschreitet, werden nur die K wahrscheinlichsten Token genommen. Top-p wird normalerweise bevorzugt, da es dem Modell ermöglicht, den Suchradius zu „tunen“, aber top-k kann als Notbremse nützlich sein, wenn das Modell keine Ahnung hat, was es als nächstes generieren soll, und vielen Token eine sehr gleichmäßige Verteilung zuweist.",
    "top_p": "Top-p-Sampling (auch bekannt als Nucleus-Sampling): Diese Methode reduziert die Wahrscheinlichkeitsverteilung, um nur die Top-p-Prozent von Token zu betrachten. Durch das Verwerfen von Token mit geringer Wahrscheinlichkeit hilft es, die Generation zu begrenzen und zu verhindern, dass das Modell grammatikalisch falsche Sätze generiert.",
    "typical_p": "Typisches p: Typisches Sampling ist ein informationstheoretisches Verfahren, das neben der Wahrscheinlichkeit auch die Sequenzentropie (also den Informationsgehalt gemäß der Wahrscheinlichkeit) berücksichtigt. Dies bedeutet, dass beim typischen Sampling einige der Token mit geringerer Wahrscheinlichkeit  \"übergewichtet\" werden, weil sie als \"interessant\" gelten, und Token mit hoher Wahrscheinlichkeit untergewichtet werden, weil sie als \"langweilig\" gelten."
  },
  "plugin_url_placeholder": "Plugin URL eingeben",
  "plugins": "Plugins",
  "preset": "Voreinstellung",
  "preset_custom": "Benutzerdefiniert",
  "queue_info": "Ihre Nachricht ist in der Warteschlange, Sie sind an {{ queuePosition, number, integer }} Stelle in der Warteschlange.",
  "remove_plugin": "Plugin löschen",
  "repetition_penalty": "Wiederholungsstrafe",
  "temperature": "Temperatur",
  "top_k": "Top K",
  "top_p": "Top P",
  "typical_p": "Typisches P",
  "unverified_plugin": "UNVERIFIZIERT",
  "unverified_plugin_description": "Dieses Plugin wurde nicht vom Open Assistant-Team überprüft. Die Verwendung erfolgt auf eigene Gefahr.",
  "used": "Verwendet",
  "verified_plugin": "VERIFIZIERT",
  "verified_plugin_description": "Dieses Plugin wurde vom Open Assistant-Team überprüft.",
  "view_plugin": "Plugin anzeigen",
  "visible_hidden": "Sichtbar & Versteckt",
  "warning": "Bei diesem Assistenten handelt es sich um eine Demonstrationsversion, die keinen Internetzugang hat. Er kann falsche oder irreführende Informationen liefern. Der Assistent ist nicht für wichtige Anwendungsfälle oder für die Erteilung von Ratschlägen geeignet.",
  "you_are_logged_in": "Sie sind am Chatservice angemeldet",
  "your_chats": "Ihre Chats",
  "save_preset": "Diese Vorlage speichern",
  "preset_exists_error": "Eine Vorlage mit diesem Namen existiert bereits",
  "preset_name_placeholder": "Namen eingeben",
  "feedback_message": "Wie war ich? Dein Feedback macht mich besser!",
  "feedback_action_great": "Gut",
  "feedback_action_poor": "Könnte besser sein"
}
