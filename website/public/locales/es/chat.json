{
  "back_to_chat_list": "Volver a la lista de chat",
  "chat_date": "{{val, datetime}}",
  "config_title": "Configuración del chat",
  "empty": "(vacío)",
  "login_message": "Para usar esta función, debe iniciar sesión nuevamente. Inicie sesión con uno de estos proveedores:",
  "max_new_tokens": "Número máximo de tokens nuevos",
  "model": "Modelo",
  "parameter_description": {
    "max_new_tokens": "Número máximo de tokens nuevos. Este parámetro indica al modelo cuántos nuevos tokens debe generar como máximo para la respuesta.",
    "repetition_penalty": "Penalización por repetición: este parámetro reduce la probabilidad de repetir los mismos tokens una y otra vez, haciendo que los tokens repetidos sean menos probables de lo que el modelo normalmente prediciría.",
    "temperature": "Temperatura: cada token que se genera se selecciona mediante una distribución p(next_token|previous_tokens). El parámetro de temperatura puede \"afilar\" o amortiguar esta distribución. Si se establece en 1, el modelo genera tokens basándose en su probabilidad predicha (es decir, si el modelo predice que \"XYZ\" tiene una probabilidad del 12,3%, lo generará con una probabilidad del 12,3%). Reducir la temperatura hacia cero hace que el modelo sea más voraz, aumentando aún más las probabilidades altas y disminuyendo aún más las probabilidades bajas (ten en cuenta que no es una relación lineal). Aumentar la temperatura hace que todas las probabilidades sean más similares. De manera intuitiva, una temperatura baja significa que el modelo genera respuestas que se alinean estrechamente con sus creencias, mientras que una temperatura alta permite respuestas más creativas y diversas.",
    "top_k": "Top K: esta opción es similar al muestreo top-p, pero en lugar de tomar los tokens principales hasta que su probabilidad acumulada supere 'p', solo toma los K tokens más probables. Suele preferirse top-p porque le permite al modelo \"ajustar\" el radio de búsqueda, pero top-k puede ser útil como un freno de emergencia cuando el modelo no sabe qué generar a continuación y asigna una distribución muy uniforme a muchos tokens.",
    "top_p": "Top P (también conocido como muestreo con núcleo): este método reduce la distribución de probabilidad para que solo tenga en cuenta el porcentaje de tokens más altos. Al descartar los tokens de baja probabilidad, ayuda a limitar la generación y evitar que el modelo genere frases gramaticalmente incorrectas.",
    "typical_p": "P típico: el muestreo típico es una técnica de teoría de la información que, además de la probabilidad, también considera la entropía de la secuencia (es decir, el contenido de información según la probabilidad). Esto significa que el muestreo típico \"sobrevalora\" algunos de los tokens con menor probabilidad porque se consideran \"interesantes\" y subestima los tokens de alta probabilidad porque se consideran \"aburridos\"."
  },
  "preset": "Preestablecido",
  "preset_custom": "Personalizado",
  "queue_info": "Tu mensaje está en cola, estás en la posición {{ queuePosition, number, integer }} en la cola.",
  "repetition_penalty": "Penalización por repetición",
  "temperature": "Temperatura",
  "top_k": "Top K",
  "top_p": "Top P",
  "typical_p": "P típico",
  "you_are_logged_in": "Ha iniciado sesión en el servicio de chat",
  "your_chats": "Tus chats"
}
