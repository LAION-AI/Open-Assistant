{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/Bobak/Documents/Work/Open-Assistant/model/reward/rankgen/.venv/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from rankgen import RankGenCollator, RankGenModel\n",
    "import wandb\n",
    "import torch\n",
    "import omegaconf\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "from typing import Literal\n",
    "import tqdm\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "from datasets import load_dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbobakhashemi\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/Bobak/Documents/Work/Open-Assistant/model/reward/rankgen/wandb/run-20221231_222326-1k9p1cs2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/bobakhashemi/open-assistant-model-reward-rankgen/runs/1k9p1cs2\" target=\"_blank\">sleek-wind-28</a></strong> to <a href=\"https://wandb.ai/bobakhashemi/open-assistant-model-reward-rankgen\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/bobakhashemi/open-assistant-model-reward-rankgen/runs/1k9p1cs2?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f345af36dd0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config =  OmegaConf.create({\"project\": \"open-assistant-model-reward-rankgen\",\n",
    "                      \"num_epochs\": 10,\n",
    "                      \"rankgen_model\": {\n",
    "                          \"rankgen_hf_path\" : \"kalpeshk2011/rankgen-t5-base-all\",\n",
    "                          # \"rankgen_hf_path\" : \"kalpeshk2011/rankgen-t5-large-all\",\n",
    "                          # \"rankgen_hf_path\" : \"kalpeshk2011/rankgen-t5-xl-all\",\n",
    "                          # \"rankgen_hf_path\" : \"kalpeshk2011/rankgen-t5-xl-pg19\",\n",
    "                          \"model_size\" : None,\n",
    "                          \"cache_dir\" : None,\n",
    "                          \"eval_mode\": True,\n",
    "                          \"snapshot_dir\": \"snapshots\",\n",
    "                          \"save_dir\": \"pretrained_models\",\n",
    "                          \"save_freq\": 2,\n",
    "                          \"save_on_best\": True,\n",
    "                          \"lr\": 1e-4,\n",
    "                        },\n",
    "                      \"dataset\": {\n",
    "                          \"name\": \"openai/webgpt_comparisons\",\n",
    "                          # \"name\": \"imdb\",\n",
    "                          # \"name\": \"summarize-from-feedback\",\n",
    "                          \"shuffle\": True,\n",
    "                          \"train_batch_size\": 24,\n",
    "                          \"max_sentence_length\": 256,\n",
    "                        }\n",
    "                      })\n",
    "wandb.init(project=\"open-assistant-model-reward-rankgen\", config=OmegaConf.to_container(config))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset webgpt_comparisons (/home/bobak/.cache/huggingface/datasets/openai___webgpt_comparisons/default/0.0.0/8b5d5879cdc98c4c0099af6053dffe8d504588d43d3b11f1b1ec223ab1e8db0a)\n",
      "100%|██████████| 1/1 [00:00<00:00, 15.04it/s]\n"
     ]
    }
   ],
   "source": [
    "full_dataset : DatasetDict = load_dataset(config.dataset.name)\n",
    "td = full_dataset[\"train\"].remove_columns(['quotes_0', 'tokens_0', 'quotes_1', 'tokens_1'])\n",
    "\n",
    "# train/test/valid split the dataset['train']\n",
    "dataset_ = td.train_test_split(test_size=0.2, train_size=0.8, shuffle=True)\n",
    "# add a validation split\n",
    "dataset_test_valid = dataset_['test'].train_test_split(test_size=0.75, train_size=0.25, shuffle=True)\n",
    "dataset = DatasetDict({\n",
    "    'train': dataset_['train'],\n",
    "    'valid': dataset_test_valid['train'],\n",
    "    'test': dataset_test_valid['test'],\n",
    "})\n",
    "del dataset_, full_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_criterion(positive_scores:  torch.Tensor, negative_scores: torch.Tensor) -> torch.Tensor:\n",
    "  return torch.sum(-torch.log(torch.sigmoid(positive_scores - negative_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetHandler:\n",
    "  def __init__(self, config: DictConfig):\n",
    "    if config.dataset.name == \"overfit-random\":\n",
    "      self.init_overfit_random(config)\n",
    "    elif config.dataset.name == \"openai/webgpt_comparisons\":\n",
    "      self.init_webgpt(config)\n",
    "    elif config.dataset.name == \"summarize-from-feedback\":\n",
    "      self.init_summarize_from_feedback(config)\n",
    "    else:\n",
    "      raise NotImplementedError(f\"Dataset {config.dataset.name} not implemented\")\n",
    "  \n",
    "  def init_webgpt(self, config: DictConfig):\n",
    "    full_dataset : DatasetDict = load_dataset(config.dataset.name)\n",
    "    \n",
    "    #columns = ['question', 'answer_0', 'score_0', 'answer_1', 'score_1']\n",
    "    td = full_dataset[\"train\"].remove_columns(['quotes_0', 'tokens_0', 'quotes_1', 'tokens_1'])\n",
    "    # train/test/valid split the dataset['train']\n",
    "    dataset_ = td.train_test_split(test_size=0.2, train_size=0.8, shuffle=True)\n",
    "    # add a validation split\n",
    "    \n",
    "    dataset_test_valid = dataset_['test'].train_test_split(test_size=0.75, train_size=0.25, shuffle=True)\n",
    "    dataset = DatasetDict({\n",
    "        'train': dataset_['train'],\n",
    "        'valid': dataset_test_valid['train'],\n",
    "        'test': dataset_test_valid['test'],\n",
    "    })\n",
    "    del dataset_, full_dataset\n",
    "    self.dataset = dataset\n",
    "    self.dataloaders = {\n",
    "      \"train\" : DataLoader(dataset[\"train\"], batch_size=config.dataset.train_batch_size, shuffle=config.dataset.shuffle, collate_fn=RankGenCollator(config)),\n",
    "      \"valid\" : DataLoader(dataset[\"valid\"], batch_size=config.dataset.train_batch_size, shuffle=config.dataset.shuffle, collate_fn=RankGenCollator(config)),\n",
    "      \"test\" : DataLoader(dataset[\"test\"], batch_size=config.dataset.train_batch_size, shuffle=config.dataset.shuffle, collate_fn=RankGenCollator(config)),\n",
    "    }\n",
    "  \n",
    "  def __repr__(self):\n",
    "    return f\"\"\"DatasetHandler:\n",
    "Dataset: {self.dataset}) \n",
    "Dataloaders: {self.dataloaders}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RankGenTrainer():\n",
    "  def __init__(self, config: DictConfig) -> None:\n",
    "    self.rankgen_model : RankGenModel = RankGenModel(config=config)\n",
    "    self.config = config\n",
    "    self.criterion = reward_criterion\n",
    "    self.data = DatasetHandler(config)\n",
    "    self.best_valid_loss = float(\"inf\")\n",
    "    self.optimizer = torch.optim.Adam(self.rankgen_model.parameters(), lr=config.rankgen_model.lr)\n",
    "    \n",
    "    self.save_dir = Path(config.rankgen_model.save_dir)\n",
    "    self.snapshot_dir = Path(config.rankgen_model.snapshot_dir)\n",
    "    if not self.save_dir.exists():\n",
    "      self.save_dir.mkdir(parents=True)\n",
    "    if not self.snapshot_dir.exists():\n",
    "      self.snapshot_dir.mkdir(parents=True)\n",
    "  \n",
    "  def train(self) -> None:\n",
    "    for epoch in range(self.config.num_epochs):\n",
    "      for batch in tqdm.tqdm(self.data.dataloaders[\"train\"], desc=f\"Epoch {epoch} -- Batch\", leave=False, total=len(self.data.dataloaders[\"train\"])):\n",
    "        prefixes, pos_suffixes, neg_suffixes = batch\n",
    "        pos_scores = self.rankgen_model(prefixes, pos_suffixes)\n",
    "        neg_scores = self.rankgen_model(prefixes, neg_suffixes)\n",
    "        \n",
    "        loss = self.criterion(pos_scores, neg_scores)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.optimizer.zero_grad()\n",
    "        wandb.log({\"train_loss\": loss.item()})\n",
    "      if self.config.rankgen_model.save_freq > 0 and epoch % self.config.rankgen_model.save_freq == 0:\n",
    "        self.save(f\"train_epoch_{epoch}\")\n",
    "      \n",
    "      self.test(\"valid\")\n",
    "      self.test(\"test\")\n",
    "    self.save(\"final\")\n",
    "    \n",
    "  def test(self, split : Literal[\"valid\", \"test\"]) -> None:\n",
    "    with torch.inference_mode():\n",
    "      for batch in self.data.dataloaders[split]:\n",
    "        prefixes, pos_suffixes, neg_suffixes = batch\n",
    "        pos_scores = self.rankgen_model(prefixes, pos_suffixes)\n",
    "        neg_scores = self.rankgen_model(prefixes, neg_suffixes)\n",
    "        \n",
    "        loss = self.criterion(pos_scores, neg_scores)\n",
    "        wandb.log({f\"{split}_loss\": loss.item()})\n",
    "        if split==\"valid\" and loss < self.best_valid_loss:\n",
    "          self.best_valid_loss = loss\n",
    "          self.save(\"best\")\n",
    "  \n",
    "  def save(self, prefix=\"best\") -> None:\n",
    "    pickle.dump(self.rankgen_model, open(self.save_dir / f\"{prefix}_rankgen_model.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Found cached dataset webgpt_comparisons (/home/bobak/.cache/huggingface/datasets/openai___webgpt_comparisons/default/0.0.0/8b5d5879cdc98c4c0099af6053dffe8d504588d43d3b11f1b1ec223ab1e8db0a)\n",
      "100%|██████████| 1/1 [00:00<00:00, 345.69it/s]\n"
     ]
    }
   ],
   "source": [
    "a = RankGenTrainer(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing basic functionality of the Rankgen Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities, _, _ = a.model.score([\"How's this?\", \"It's crazy when a best friend finishes your\"], [\"It's actually pretty good\", \"sentences\", \"we were at the park\"])\n",
    "print(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in dataset[\"train\"]:\n",
    "  print(x)\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5b0eb2bbebb7cf98106a88f983bb9b665d4e8eec4abae03639502717fa636ccd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
