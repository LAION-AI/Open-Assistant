model_name: kalpeshk2011/rankgen-t5-base-all
tokenizer_name: google/t5-v1_1-base
learning_rate: 6e-6
gradient_checkpointing: false
fp16: true
gradient_accumulation_steps: 16
per_device_train_batch_size: 2
warmup_steps: 600
freeze_layer: 20
eval_steps: 200
save_steps: 500
max_length: 400
num_train_epochs: 2
datasets:
  - webgpt
  - hfsummary
