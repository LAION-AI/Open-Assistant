model_name: facebook/galactica-125m
learning_rate: 1e-5
gradient_checkpointing: false
gradient_accumulation_steps: 10
per_device_train_batch_size: 6
warmup_steps: 600
loss: cls
eval_steps: 200
save_steps: 500
max_length: 128
num_train_epochs: 2
datasets:
  - webgpt
  - hfsummary
