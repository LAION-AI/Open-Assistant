defaults_rlhf:
  rng_seed: 0xa1221f97
  dataset:
  rank_model: TODO
  sft_model: TODO
  eval_prompts:
  batch_size: 18
  epochs: 10
  datasets:
    - oa_private:
        data_path: .cache
        split: rl
        val_split: 0.0
        fraction: 1
        file: 2023-02-12_oasst_prod.jsonl
  cache_dir: .cache
  quantization: false
  seq2seqmodel: false

pythia_2b_rlhf:
  model_name: EleutherAI/pythia-1b-deduped-base-finetuned/checkpoint-2000
  rank_model: ../reward/instructor/microsoft/deberta-v3-large-finetuned/checkpoint-4500
  sft_model: EleutherAI/pythia-1b-deduped-base-finetuned/checkpoint-2000
  batch_size: 18

debug_rlhf:
  model_name: gpt2
  rank_model: /local/home/sanagnos/general/Open-Assistant/model/reward/instructor/facebook/galactica-125m-finetuned/checkpoint-500/
  sft_model: /local/home/sanagnos/general/Open-Assistant/model/model_training/EleutherAI/pythia-70m-deduped-base-finetuned/checkpoint-20/
  batch_size: 2
