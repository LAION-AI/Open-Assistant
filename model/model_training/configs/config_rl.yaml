defaults_rlhf:
  dataset:
  rank_model: TODO
  sft_model: TODO
  eval_prompts:
  batch_size: 64
  epochs: 10
  datasets:
    - oa_private:
        data_path: .cache
        split: rl
        val_split: 0.0
        fraction: 1
        file: 2023-02-10_oasst_prod.jsonl
  cache_dir: .cache
  quantization: false
  seq2seqmodel: false

debug_rlhf:
  model_name: gpt2
  rank_model: /local/home/sanagnos/general/Open-Assistant/model/reward/instructor/facebook/galactica-125m-finetuned/checkpoint-500/
  sft_model: /local/home/sanagnos/general/Open-Assistant/model/model_training/EleutherAI/pythia-70m-deduped-base-finetuned/checkpoint-20/
  batch_size: 2
