defaults_rlhf:
  rng_seed: 0xa1221f97
  datasets: []
  batch_size: 1
  chunk_size: 1
  num_rollouts: 16
  total_steps: 10000
  datasets_extra: []
  cache_dir: /home/ubuntu/data_cache
  output_dir: model_rl
  eval_size: 500
  num_eval_prompts: 64
  rank_config:
  sft_config:
  debug: false
  dtype: fp16

oasst_export_latin_cyrillic_rlhf:
  datasets:
    - oasst_export:
        #lang: "bg,ca,cs,da,de,en,es,fr,hr,hu,it,nl,pl,pt,ro,ru,sl,sr,sv,uk"
        lang: "en"
        #top_k: 2
        #input_file_path: 2023-03-25_oasst_research_ready_synth_labels.jsonl.gz
        input_file_path: 2023-04-12_oasst_all.trees.jsonl.gz
  sort_by_length: false
  use_custom_sampler: false

oa_df:
  datasets:
    - oasst_export:
        lang: "en"
        hf_dataset_name: toanbku/oa-df
  sort_by_length: false
  use_custom_sampler: false

pythia_rlhf:
  triton_host_rm: localhost:8002/toanbku-oa-rm-2.1-pythia-1.4b-df
  triton_host_sft: localhost:8005/toanbku-oa-falcon-7b-sft-df
  rank_config:
    is_reward_model: true
    model_name: toanbku/oa-rm-2.1-pythia-1.4b-df # just for tokenizer...andreaskoepf/oasst-rm-1-pythia-1b
    cache_dir: /home/ubuntu/OA/model/model_training/.cache
    pooling: last
    residual_dropout: 0.01
    use_flash_attention: false
    dtype: float32
    batch_size: 2
    residual_dropout_lima: false

  sft_config:
    is_reward_model: false
    model_name: toanbku/oa-falcon-7b-sft-df
    cache_dir: /home/ubuntu/OA/model/model_training/.cache
    quantization: false
    seq2seqmodel: false
    freeze_layer:
    num_layers_unfrozen: -1
    residual_dropout: 0.2
    use_flash_attention: false
    dtype: fp16
    batch_size: 1
    peft_type: "lora"
    residual_dropout_lima: false

llama_rlhf:
  triton_host_rm: localhost:8002/OpenAssistant-oasst-rm-2-pythia-6.9b-epoch-1
  triton_host_sft: localhost:8005/OpenAssistant-oasst-sft-7e2-llama-30b
  rank_config:
    is_reward_model: true
    model_name: OpenAssistant/oasst-rm-2-pythia-6.9b-epoch-1
    cache_dir: /mnt/data/Open-Assistant-RLHF/model/model_training/.cache
    pooling: last
    residual_dropout: 0.0
    use_flash_attention: false
    dtype: fp16
    batch_size: 8

  sft_config:
    is_reward_model: false
    model_name: OpenAssistant/oasst-sft-7e2-llama-30b
    cache_dir: /mnt/data/Open-Assistant-RLHF/model/model_training/.cache
    quantization: false
    seq2seqmodel: false
    freeze_layer: 52
    num_layers_unfrozen: -1 # we don't use this, trlx has its own implementation
    residual_dropout: 0.0
    use_flash_attention: true
    dtype: fp16
    peft_type: "lora"

debug_rlhf:
  rank_model: pythia_reward_model/checkpoint-50
  sft_model: pythia_sft/checkpoint-10/
  batch_size: 2
  log_dir: test
