{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "collapsed_sections": [
    "5MUlZr46cYwn",
    "dkVFJcUln63N"
   ],
   "authorship_tag": "ABX9TyP7XYQu9vBEThZ072LOnnZE",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "gpuClass": "standard",
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/horribleCodes/NLP-tools/blob/main/data-processors/QA_Generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# QA Generator\n",
    "\n",
    "Creates a set of questions and answers to a given paragraph. Allows for sample topics, questions and answers for few-shot examples.\n",
    "A JSON input file with the following structure is required: (`paragraph` is the only required tag.)\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"paragraphs\":\n",
    "  [{\n",
    "    \"text\": \"\",\n",
    "    \"topics\": [\"\"],\n",
    "    \"questions\": [\"\"],\n",
    "    \"answers\": [\"\"]\n",
    "  }]\n",
    "}\n",
    "```\n",
    "\n",
    "This notebook will run on a system with a single RTX3090 (24 GB vram) GPU. If you're using Colab, don't forget to change the Runtime to GPU-accelerated!\n",
    "\n",
    "Inference code and structure provided by @ontocord\n",
    "\n",
    "## Parameters\n",
    "\n",
    "(Only required to run once)"
   ],
   "metadata": {
    "id": "Asulwwv3wZ39"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model_hf_name = \"google/flan-t5-large\"  # @param {type:\"string\"}\n",
    "verbose = True  # @param {type:\"boolean\"}\n",
    "file_path = \"/content/paragraphs.json\"  # @param {type:\"string\"}\n",
    "output_path = \"/content/questions_dict.json\"  # @param {type:\"string\"}"
   ],
   "metadata": {
    "cellView": "form",
    "id": "CSG86OEbkwsn"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup\n",
    "\n",
    "(Only required to run once)"
   ],
   "metadata": {
    "id": "TN0dDoNAngUM"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Setup the necessary libraries and dictionaries"
   ],
   "metadata": {
    "id": "5MUlZr46cYwn"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Install with pip\n",
    "!pip install accelerate\n",
    "!pip install bitsandbytes\n",
    "!pip install transformers"
   ],
   "metadata": {
    "id": "GttFRKzOuDcp"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Load all necessary libraries\n",
    "import math\n",
    "import pickle\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "import json\n",
    "\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, AutoModel\n",
    "from torch.nn.functional import cosine_similarity\n",
    "\n",
    "# This device map will work a GPU with > 24GB vram. It uses nearly all memory.\n",
    "device_map_T5_13B = {\n",
    "    \"shared\": 0,\n",
    "    \"decoder.embed_tokens\": 0,\n",
    "    \"encoder.embed_tokens\": 0,\n",
    "    \"encoder.block.0\": 0,\n",
    "    \"encoder.block.1\": 0,\n",
    "    \"encoder.block.2\": 0,\n",
    "    \"encoder.block.3\": 0,\n",
    "    \"encoder.block.4\": 0,\n",
    "    \"encoder.block.5\": 0,\n",
    "    \"encoder.block.6\": 0,\n",
    "    \"encoder.block.7\": 0,\n",
    "    \"encoder.block.8\": 0,\n",
    "    \"encoder.block.9\": 0,\n",
    "    \"encoder.block.10\": 0,\n",
    "    \"encoder.block.11\": 0,\n",
    "    \"encoder.block.12\": 0,\n",
    "    \"encoder.block.13\": 0,\n",
    "    \"encoder.block.14\": 0,\n",
    "    \"encoder.block.15\": 0,\n",
    "    \"encoder.block.16\": 0,\n",
    "    \"encoder.block.17\": 0,\n",
    "    \"encoder.block.18\": 0,\n",
    "    \"encoder.block.19\": 0,\n",
    "    \"encoder.block.20\": 0,\n",
    "    \"encoder.block.21\": 0,\n",
    "    \"encoder.block.22\": 0,\n",
    "    \"encoder.block.23\": 0,\n",
    "    \"encoder.final_layer_norm\": 0,\n",
    "    \"encoder.dropout\": 0,\n",
    "    \"decoder.block.0\": 0,\n",
    "    \"decoder.block.1\": 0,\n",
    "    \"decoder.block.2\": 0,\n",
    "    \"decoder.block.3\": 0,\n",
    "    \"decoder.block.4\": 0,\n",
    "    \"decoder.block.5\": 0,\n",
    "    \"decoder.block.6\": 0,\n",
    "    \"decoder.block.7\": 0,\n",
    "    \"decoder.block.8\": 0,\n",
    "    \"decoder.block.9\": 0,\n",
    "    \"decoder.block.10\": 0,\n",
    "    \"decoder.block.11\": 0,\n",
    "    \"decoder.block.12\": 0,\n",
    "    \"decoder.block.13\": 0,\n",
    "    \"decoder.block.14\": 0,\n",
    "    \"decoder.block.15\": 0,\n",
    "    \"decoder.block.16\": 0,\n",
    "    \"decoder.block.17\": 0,\n",
    "    \"decoder.block.18\": 0,\n",
    "    \"decoder.block.19\": 0,\n",
    "    \"decoder.block.20\": 0,\n",
    "    \"decoder.block.21\": 0,\n",
    "    \"decoder.block.22\": 0,\n",
    "    \"decoder.block.23\": 0,\n",
    "    \"decoder.final_layer_norm\": 0,\n",
    "    \"decoder.dropout\": 0,\n",
    "    \"lm_head\": 0,\n",
    "}"
   ],
   "metadata": {
    "id": "Vy9JKO8pKda8"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load Large Language Model from Hugging Face"
   ],
   "metadata": {
    "id": "xu1j3-jdcKyx"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Load the model in bfloat16. Make sure to use bfloat16\n",
    "# if you are doing inference with 16bit precision.\n",
    "try:\n",
    "    if tokenizer is not None:\n",
    "        pass\n",
    "except:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_hf_name)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        model_hf_name,\n",
    "        device_map=device_map_T5_13B,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        load_in_8bit=False,\n",
    "    )\n",
    "    minilm_tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    minilm_model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\").half().eval().cuda()"
   ],
   "metadata": {
    "id": "0Yn7IO8pJi99"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define the functions"
   ],
   "metadata": {
    "id": "XheTN3ixcDu1"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Inference\n",
    "\n",
    "\n",
    "def Write_Line(name, value):\n",
    "    name_stripped = name.strip\n",
    "    value_stripped = value.strip\n",
    "    output = \"{}:\\n{}\\n\"\n",
    "    return output.format(name_stripped, value_stripped)\n",
    "\n",
    "\n",
    "# ask_flan_T5 takes a text input and returns the\n",
    "# response of FLAN_T5 and a normalized logits\n",
    "# score for the generation.\n",
    "# Input: input_text (string): A string used as the prompt directed to the model\n",
    "# Output: out_tuple (tuple): A list of string-float pairs that contain results and the normalized logit\n",
    "def ask_flan_T5(input_text):\n",
    "    inputs = tokenizer.encode(input_text, return_tensors=\"pt\").cuda(0)\n",
    "    outputs = model.generate(\n",
    "        inputs,\n",
    "        do_sample=True,\n",
    "        top_p=0.95,\n",
    "        eos_token_id=1,\n",
    "        max_new_tokens=50,\n",
    "        bos_token_id=0,\n",
    "        temperature=0.9,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True,\n",
    "    )\n",
    "    out_text = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n",
    "    probs = torch.stack(outputs.scores, dim=1).softmax(-1)\n",
    "    for i in outputs.sequences:\n",
    "        logprobs = 0\n",
    "        counter = 0\n",
    "        for k in i[1:]:\n",
    "            word_prob = (round(probs[0][counter][k.item()].item(), 2)) + 0.001\n",
    "            logprobs = logprobs + math.log(word_prob)\n",
    "            counter += 1\n",
    "        out_tuple = (out_text, round(logprobs, 2))\n",
    "    return out_tuple\n",
    "\n",
    "\n",
    "# ask_flan_T5D is a function that takes an input text and\n",
    "# returns the deterministic(do_sample=False) output of\n",
    "# FLAN_T5 and logits.\n",
    "def ask_flan_T5D(input_text):\n",
    "    inputs = tokenizer.encode(input_text, return_tensors=\"pt\").cuda(0)\n",
    "    outputs = model.generate(\n",
    "        inputs,\n",
    "        do_sample=False,\n",
    "        eos_token_id=1,\n",
    "        max_new_tokens=50,\n",
    "        bos_token_id=0,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True,\n",
    "    )\n",
    "    out_text = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n",
    "    probs = torch.stack(outputs.scores, dim=1).softmax(-1)\n",
    "    for i in outputs.sequences:\n",
    "        logprobs = 0\n",
    "        counter = 0\n",
    "        for k in i[1:]:\n",
    "            word_prob = (round(probs[0][counter][k.item()].item(), 2)) + 0.001\n",
    "            logprobs = logprobs + math.log(word_prob)\n",
    "            counter += 1\n",
    "        out_tuple = (out_text, round(logprobs, 2))\n",
    "    return out_tuple"
   ],
   "metadata": {
    "id": "6Fg6Eh-QfR7a"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Topics\n",
    "\n",
    "\n",
    "def Get_Topics(paragraph_dict):\n",
    "    output_dict = {}\n",
    "    for id in paragraph_dict:\n",
    "        topic_list = Get_Topic(paragraph_dict[id])\n",
    "        output_dict[id] = topic_list\n",
    "    return output_dict\n",
    "\n",
    "\n",
    "def Get_Topic(paragraph_item):\n",
    "    topic_list = generate_topic(paragraph_item)\n",
    "    return topic_list\n",
    "\n",
    "\n",
    "# Generate a topic classifier for a paragraph of text\n",
    "def generate_topic(paragraph):\n",
    "    paragraph_text = paragraph[\"paragraph\"]\n",
    "    sample_topics = paragraph[\"sample topics\"]\n",
    "\n",
    "    samples = \"\"\n",
    "    if len(sample_topics) > 0:\n",
    "        k = random.randint(0, len(sample_topics) - 1)\n",
    "        samples += Write_Line(\"Topic\", sample_topics[k])\n",
    "\n",
    "    results = set()\n",
    "    input_text = (\n",
    "        \"Task: Create a topic classifier for the provided paragraph.\\\n",
    "    \\nParagraph:\\n\"\n",
    "        + paragraph_text\n",
    "        + \"\\n\"\n",
    "        + samples\n",
    "        + \"Topic:\\n\"\n",
    "    )\n",
    "\n",
    "    for k in range(0, 20):\n",
    "        result = ask_flan_T5(input_text)\n",
    "        if result[1] > -4:\n",
    "            results.add(result)\n",
    "    if len(results) < 3:\n",
    "        results.add((\"I was wondering\", -3.3))\n",
    "        results.add((\"I have a question\", -3.3))\n",
    "\n",
    "    sorted_results = Sort_Tuple(list(results))\n",
    "    return sorted_results[0:5]"
   ],
   "metadata": {
    "id": "30WgsK2LfvW5"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Prefixes\n",
    "\n",
    "\n",
    "def Get_Prefixes(paragraph_dict):\n",
    "    output_dict = {}\n",
    "    for id in paragraph_dict:\n",
    "        prefix_list = Get_Prefix(paragraph_dict, id)\n",
    "        output_dict[id] = prefix_list\n",
    "    return output_dict\n",
    "\n",
    "\n",
    "def Get_Prefix(paragraph_dict, id):\n",
    "    prefix_list = generate_topic_prefix(paragraph_dict[id])\n",
    "    return prefix_list\n",
    "\n",
    "\n",
    "# Generate a topic classifier for a paragraph of text\n",
    "def generate_topic_prefix(topic_set):\n",
    "    results = set()\n",
    "    for entry in topic_set:\n",
    "        topic = entry[0]\n",
    "        input_text = (\n",
    "            \"Task: Create a prepositional phrase about the topic.\\n\\\n",
    "      Example 1\\n Topic: Climbing Mount Everest\\nPrepositional \\\n",
    "      Phrase: With regards to climbing Mount Everest,\\nExample \\\n",
    "      2\\nTopic: United States Air Force\\nPrepositional Phrase: \\\n",
    "      On the topic of the United States Air Force,\\n Example 3\\nTopic: \"\n",
    "            + topic\n",
    "            + \"\\nPrepositional Phrase: \"\n",
    "        )\n",
    "        for k in range(0, 5):\n",
    "            results.add(ask_flan_T5(input_text))\n",
    "\n",
    "        sorted_results = Sort_Tuple(list(results))\n",
    "        return sorted_results[0:5]"
   ],
   "metadata": {
    "id": "92VQci_Lf00B"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Questions\n",
    "\n",
    "\n",
    "def Get_Questions(paragraph_dict, number_of_questions):\n",
    "    output_dict = {}\n",
    "    for id in paragraph_dict:\n",
    "        question_list = Get_Question(paragraph_dict, id, number_of_questions)\n",
    "        output_dict[id] = question_list\n",
    "    return output_dict\n",
    "\n",
    "\n",
    "def Get_Question(paragraph_dict, id, number_of_questions):\n",
    "    question_list = generate_questions(paragraph_dict[id], number_of_questions)\n",
    "    return question_list\n",
    "\n",
    "\n",
    "# Generate who/what/where/when/why questions from a paragraph.\n",
    "# Number of questions variable is an integer which indicates how\n",
    "# many of each question type to try to generate.\n",
    "def generate_questions(paragraph, number_of_questions):\n",
    "    paragraph_text = paragraph[\"paragraph\"]\n",
    "\n",
    "    if len(tokenizer.encode(paragraph_text)) > 480:\n",
    "        print(\"Warning, the context length is too long.\")\n",
    "    question_set = set()\n",
    "    question_types = [\"What\", \"Where\", \"Why\", \"How\", \"How much\", \"Who\", \"When\", \"Which\"]\n",
    "    for qtype in question_types:\n",
    "        question = (\n",
    "            \"Please generate a question that starts with '\"\n",
    "            + qtype\n",
    "            + \"' based on the following paragraph.\\nText:\\n\"\n",
    "            + paragraph_text\n",
    "            + \"\\nQuestion:\\n\"\n",
    "        )\n",
    "        for k in range(0, number_of_questions):\n",
    "            new_question = ask_flan_T5(question)\n",
    "            if qtype in new_question[0]:\n",
    "                question_set.add((qtype, new_question))\n",
    "    return question_set"
   ],
   "metadata": {
    "id": "eBR3Lhysf4jz"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Answers\n",
    "\n",
    "\n",
    "def Get_Answers(paragraph_dict, question_dict):\n",
    "    output_dict = {}\n",
    "    for id in paragraph_dict:\n",
    "        answer_list = Get_Answer(paragraph_dict, id, question_dict[id])\n",
    "        output_dict[id] = answer_list\n",
    "    return output_dict\n",
    "\n",
    "\n",
    "def Get_Answer(paragraph_dict, id, question_list):\n",
    "    answer_list = generate_answers(paragraph_dict[id], question_list)\n",
    "    return answer_list\n",
    "\n",
    "\n",
    "# Generate answers for a set of questions.\n",
    "# Input is the paragraph of text and a set of questions where each question\n",
    "# is a tuple generated from the generate_questions() function.\n",
    "def generate_answers(paragraph, question_set):\n",
    "    paragraph_text = paragraph[\"paragraph\"]\n",
    "    sample_questions = paragraph[\"sample questions\"]\n",
    "    sample_answers = paragraph[\"sample answers\"]\n",
    "\n",
    "    possible_answers = set()\n",
    "    for question in question_set:\n",
    "        samples = \"\"\n",
    "\n",
    "        if len(sample_questions) > 0:\n",
    "            k = random.randint(0, len(sample_questions) - 1)\n",
    "            samples += Write_Line(\"Question\", sample_questions[k])\n",
    "            samples += Write_Line(\"Answer\", sample_answers[k])\n",
    "\n",
    "        input_text = (\n",
    "            \"Please read the following paragraph and \\\n",
    "      answer the question using only data \\\n",
    "      found in the text. If no answer is possible, respond \\\n",
    "      'NA'.\\nParagraph:\\n\"\n",
    "            + paragraph_text\n",
    "            + \"\\n\"\n",
    "            + samples\n",
    "            + \"Question:\\n\"\n",
    "            + question[1][0]\n",
    "            + \"\\nAnswer:\\n\"\n",
    "        )\n",
    "        answer = ask_flan_T5D(input_text)\n",
    "        possible_answers.add((question[0], question[1], answer))\n",
    "    return possible_answers"
   ],
   "metadata": {
    "id": "M5YRePLIf67T"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Deduced Questions\n",
    "\n",
    "\n",
    "def Get_Questions2(paragraph_dict, answer_dict):\n",
    "    output_dict = {}\n",
    "    for id in paragraph_dict:\n",
    "        question2_list = Get_Question2(paragraph_dict, id, answer_dict[id])\n",
    "        output_dict[id] = question2_list\n",
    "    return output_dict\n",
    "\n",
    "\n",
    "def Get_Question2(paragraph_dict, id, answer_list):\n",
    "    question2_list = generate_question2(paragraph_dict[id], answer_list)\n",
    "    return question2_list\n",
    "\n",
    "\n",
    "# Generate questions from a paragraph and set of answers.\n",
    "# Input is the paragraph of text and a set of answers where each question\n",
    "# is a tuple generated from the generate_answers() function.\n",
    "def generate_question2(paragraph, qa_set):\n",
    "    paragraph_text = paragraph[\"paragraph\"]\n",
    "    sample_questions = paragraph[\"sample questions\"]\n",
    "    sample_answers = paragraph[\"sample answers\"]\n",
    "\n",
    "    qaq_results = set()\n",
    "    for qa_item in qa_set:\n",
    "        samples = \"\"\n",
    "        if len(sample_questions) > 0:\n",
    "            k = random.randint(0, len(sample_questions) - 1)\n",
    "            samples += Write_Line(\"Answer\", sample_answers[k])\n",
    "            samples += Write_Line(\"Question\", sample_questions[k])\n",
    "\n",
    "        answer = qa_item[2][0]\n",
    "        input_text = (\n",
    "            \"Please read the following paragraph and \\\n",
    "      generate a question to the given answer.\"\n",
    "            + \"\\nParagraph:\\n\"\n",
    "            + paragraph_text\n",
    "            + \"\\n\"\n",
    "            + samples\n",
    "            + \"Answer:\\n\"\n",
    "            + answer\n",
    "            + \"\\nQuestion:\\n\"\n",
    "        )\n",
    "        result = ask_flan_T5D(input_text)\n",
    "        qaq_results.add((qa_item[0], qa_item[1], qa_item[2], result))\n",
    "    return qaq_results"
   ],
   "metadata": {
    "id": "M9JdhvCbf8oH"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Answers to Deduced Questions\n",
    "\n",
    "\n",
    "def Get_Answers2(paragraph_dict, question2_dict):\n",
    "    output_dict = {}\n",
    "    for id in paragraph_dict:\n",
    "        answer2_list = Get_Answer2(paragraph_dict, id, question2_dict[id])\n",
    "        output_dict[id] = answer2_list\n",
    "    return output_dict\n",
    "\n",
    "\n",
    "def Get_Answer2(paragraph_dict, id, question2_list):\n",
    "    answer_list = generate_answers2(paragraph_dict[id], question2_list)\n",
    "    return answer_list\n",
    "\n",
    "\n",
    "# Generate answers from a paragraph and set of questions.\n",
    "# Input is the paragraph of text and a set of questions where each answer\n",
    "# is a tuple generated from the generate_questions2() function.\n",
    "def generate_answers2(paragraph, question2_set):\n",
    "    paragraph_text = paragraph[\"paragraph\"]\n",
    "\n",
    "    possible_answers = set()\n",
    "    for qaq2_item in question2_set:\n",
    "        question2 = qaq2_item[3][0]\n",
    "        input_text = (\n",
    "            \"Please read the following paragraph and \\\n",
    "      then answer the question using only data \\\n",
    "      found in the text. If no answer is possible, respond \\\n",
    "      'NA'.\\nText:\\n\"\n",
    "            + paragraph_text\n",
    "            + \"\\nQuestion:\\n\"\n",
    "            + question2\n",
    "            + \"\\nAnswer:\\n\"\n",
    "        )\n",
    "        answer = ask_flan_T5D(input_text)\n",
    "        possible_answers.add((question2, answer))\n",
    "    return possible_answers"
   ],
   "metadata": {
    "id": "RYmQQm3Gf_Sc"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Declaratives\n",
    "\n",
    "\n",
    "def Get_Declaratives(paragraph_dict, answer2_dict):\n",
    "    output_dict = {}\n",
    "    for id in paragraph_dict:\n",
    "        declarative_list = Get_Declarative(answer2_dict[id])\n",
    "        output_dict[id] = declarative_list\n",
    "    return output_dict\n",
    "\n",
    "\n",
    "def Get_Declarative(answer2_list):\n",
    "    declarative_list = generate_declarative(answer2_list)\n",
    "    return declarative_list\n",
    "\n",
    "\n",
    "# Generate declarative statement from question and answer pair.\n",
    "def generate_declarative(qaq_set):\n",
    "    qaqd_results = set()\n",
    "    for qa_item in qaq_set:\n",
    "        question = qa_item[0]\n",
    "        answer = qa_item[1][0]\n",
    "        if \"NA\" in answer:\n",
    "            qaqd_results.add((question, answer, qa_item[1]))\n",
    "        else:\n",
    "            input_text = (\n",
    "                \"Generate a declarative statement based on the \\\n",
    "        given question and answer pair.\\nQ: What is \\\n",
    "        sitting on the couch?\\nA: poodle\\nA poodle is \\\n",
    "        sitting on the couch.\\nQ: \"\n",
    "                + question\n",
    "                + \"\\nA: \"\n",
    "                + answer\n",
    "                + \"\\n\"\n",
    "            )\n",
    "            result = ask_flan_T5D(input_text)\n",
    "            qaqd_results.add((question, answer, result))\n",
    "    return qaqd_results"
   ],
   "metadata": {
    "id": "vNTUuC7tgBOk"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Closed Answers\n",
    "def Get_Closed_Answers(paragraph_dict, question2_dict, prefix_dict):\n",
    "    output_dict = {}\n",
    "    for id in paragraph_dict:\n",
    "        try:\n",
    "            prefix_list = prefix_dict[id]\n",
    "        except Exception:\n",
    "            prefix_list = None\n",
    "        closed_answer_list = Get_Closed_Answer(question2_dict[id], prefix_list)\n",
    "        output_dict[id] = closed_answer_list\n",
    "    return output_dict\n",
    "\n",
    "\n",
    "def Get_Closed_Answer(answer_list, prefix_list):\n",
    "    closed_answer_list = generate_closed_answer(answer_list, prefix_list)\n",
    "    return closed_answer_list\n",
    "\n",
    "\n",
    "# Generate closed book answer to question.\n",
    "def generate_closed_answer(qaqd_set, topic_prefix):\n",
    "    if topic_prefix:\n",
    "        topic_prefix = [a[0] for a in topic_prefix]\n",
    "        topic_prefix.sort(key=lambda a: len(a[0]), reverse=True)\n",
    "        topic_prefix = topic_prefix[0]\n",
    "    else:\n",
    "        topic_prefix = None\n",
    "    qaqd_results = set()\n",
    "    for qa_item in qaqd_set:\n",
    "        question = qa_item[0]\n",
    "        answer = qa_item[2][0]\n",
    "        if \"NA\" in answer:\n",
    "            if len(qa_item) == 3:\n",
    "                qaqd_results.add((qa_item[0], qa_item[1], qa_item[2], qa_item[2]))\n",
    "            else:\n",
    "                qaqd_results.add((qa_item[0], qa_item[1], qa_item[2], qa_item[2], qa_item[2]))\n",
    "            pass\n",
    "        else:\n",
    "            input_text = (\n",
    "                \"Task: Answer the question in a detailed fashion. \\\n",
    "        If the question cannot be answered without more \\\n",
    "        information, please answer NA.\\nExample 1:\\nQuestion: \\\n",
    "        Why does Shala like cookies?\\nAnswer: It is not possible \\\n",
    "        to know why Shala likes cookies without more information, \\\n",
    "        but many people that like cookies enjoy their taste or \\\n",
    "        some of their ingredients (e.g. chocolate chips or \\\n",
    "        peanut butter).\\nExample 2:\\nQuestion: Why would someone \\\n",
    "        vote in an election?\\nAnswer: There are many reasons \\\n",
    "        someone might vote in an election, for instance to have \\\n",
    "        their voice heard or to help a candidate they like win the \\\n",
    "        race.\\nExample 3\\nQuestion: What decoration goes on top of \\\n",
    "        a Christmas tree?\\nAnswer: Usually a star is placed at the \\\n",
    "        top of a Christmas tree.\\nExample 4:\\nQuestion: \"\n",
    "                + (question if topic_prefix is None else (topic_prefix + \" \" + question))\n",
    "                + \"\\nAnswer: \"\n",
    "            )\n",
    "            result = ask_flan_T5D(input_text)\n",
    "            if len(qa_item) == 3:\n",
    "                qaqd_results.add((qa_item[0], qa_item[1], qa_item[2], result))\n",
    "            else:\n",
    "                qaqd_results.add((qa_item[0], qa_item[1], qa_item[2], qa_item[3], result))\n",
    "    return qaqd_results"
   ],
   "metadata": {
    "id": "9NmW-FkGgDjb"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Tools\n",
    "\n",
    "\n",
    "# Sort_Tuple sorts a list of tuples\n",
    "# by the second element.\n",
    "def Sort_Tuple(tup):\n",
    "    tup.sort(key=lambda x: x[1], reverse=True)\n",
    "    return tup\n",
    "\n",
    "\n",
    "def Lower_First_Char(input):\n",
    "    return input[0].lower() + input[1:]\n",
    "\n",
    "\n",
    "def Format_Answer(answer, score):\n",
    "    if score < 0.75:\n",
    "        output = \"I don't know. I cannot tell you the answer with the information I have.\"\n",
    "    elif score < 0.8:\n",
    "        output = \"I don't know for certain, but maybe \" + Lower_First_Char(answer)\n",
    "    elif score < 0.9:\n",
    "        output = \"I believe \" + Lower_First_Char(answer)\n",
    "    else:\n",
    "        output = answer\n",
    "    return output\n",
    "\n",
    "\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    with torch.no_grad():\n",
    "        token_embeddings = model_output.last_hidden_state\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).to(token_embeddings.dtype)\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "\n",
    "def Get_Mean_Vector(input):\n",
    "    toks = minilm_tokenizer(input, padding=True, truncation=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "    dat = minilm_model(**toks)\n",
    "    dat = mean_pooling(dat, toks.attention_mask)\n",
    "    return dat\n",
    "\n",
    "\n",
    "def Truncate_String(input, length):\n",
    "    if len(input) > length:\n",
    "        input = input[: length - 3] + \"...\"\n",
    "    return input"
   ],
   "metadata": {
    "id": "FR5pYI7z9vtW"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Dictionary Management\n",
    "\n",
    "\n",
    "# Discards paragraphs that are too long or don't have the same number of questions and answers.\n",
    "# Input: paragraphs (list): A list of dictionaries containing the text, sample questions and sample answers of a paragraph\n",
    "def Fix_Paragraphs(paragraphs):\n",
    "    fixed_paragraphs = []\n",
    "    for paragraph in paragraphs:\n",
    "        text = paragraph[\"text\"]\n",
    "        text_trunc = Truncate_String(text, 50)\n",
    "        questions = paragraph[\"questions\"]\n",
    "        answers = paragraph[\"answers\"]\n",
    "        if len(questions) != len(answers):\n",
    "            if verbose:\n",
    "                print(text_trunc, \"Questions and answers have to have the same number of items!\")\n",
    "            continue\n",
    "        if verbose:\n",
    "            print(text_trunc, len(text), \"chars\")\n",
    "        if len(text) > 0 and len(text) <= 1100:\n",
    "            fixed_paragraphs.append(paragraph)\n",
    "\n",
    "    print(\"\\nOriginal number of paragraphs:\", len(paragraphs))\n",
    "    print(\"Length filtered number of paragraphs:\", len(fixed_paragraphs))\n",
    "    return fixed_paragraphs\n",
    "\n",
    "\n",
    "def Init_Dictionary(paragraphs):\n",
    "    paragraph_dict = {}\n",
    "    uniq_id = 100000\n",
    "    for paragraph in paragraphs:\n",
    "        paragraph_dict[uniq_id] = {}\n",
    "        paragraph_dict[uniq_id][\"paragraph\"] = paragraph[\"text\"]\n",
    "        paragraph_dict[uniq_id][\"sample topics\"] = paragraph[\"topics\"]\n",
    "        paragraph_dict[uniq_id][\"sample questions\"] = paragraph[\"questions\"]\n",
    "        paragraph_dict[uniq_id][\"sample answers\"] = paragraph[\"answers\"]\n",
    "        uniq_id += 1\n",
    "    return paragraph_dict\n",
    "\n",
    "\n",
    "def Attach_Generated_Content(paragraph_dict, topic_dict, prefix_dict, prefix_answer_dict):\n",
    "    for id in paragraph_dict.keys():\n",
    "        paragraph_dict[id][\"topics\"] = topic_dict[id]\n",
    "        paragraph_dict[id][\"topic prepositions\"] = prefix_dict[id]\n",
    "        paragraph_dict[id][\"QA_set\"] = Get_QA_Dict(prefix_answer_dict[id])\n",
    "\n",
    "\n",
    "def Get_QA_Dict(prefix_answer_list):\n",
    "    k = 0\n",
    "    output_dict = {}\n",
    "    for entry in prefix_answer_list:\n",
    "        output_dict[k] = {}\n",
    "        output_dict[k][\"question\"] = entry[0]\n",
    "        output_dict[k][\"answer_T5_ob\"] = entry[2][0]\n",
    "        output_dict[k][\"answer_T5_cb\"] = entry[3][0]\n",
    "        output_dict[k][\"answer_T5_cb_with_prefix\"] = entry[4][0]\n",
    "\n",
    "        if output_dict[k][\"answer_T5_ob\"] == \"NA\":\n",
    "            output_dict[k][\n",
    "                \"answer_T5_answer\"\n",
    "            ] = \"Either I do not understand this question, or this question cannot be answered.\"\n",
    "        else:\n",
    "            answer_ob = output_dict[k][\"answer_T5_ob\"]\n",
    "            dat_ob = Get_Mean_Vector(answer_ob)\n",
    "\n",
    "            answer_cb = output_dict[k][\"answer_T5_cb\"]\n",
    "            dat_cb = Get_Mean_Vector(answer_cb)\n",
    "            score_cb = cosine_similarity(dat_ob, dat_cb).item()\n",
    "            output_dict[k][\"answer_T5_answer\"] = Format_Answer(answer_ob, score_cb)\n",
    "\n",
    "            answer_prefix_format = output_dict[k][\"answer_T5_answer\"]\n",
    "            if len(answer_cb) < len(output_dict[k][\"answer_T5_cb_with_prefix\"]):\n",
    "                answer_prefix = output_dict[k][\"answer_T5_cb_with_prefix\"]\n",
    "                dat_prefix = Get_Mean_Vector(answer_prefix)\n",
    "                score_prefix = cosine_similarity(dat_ob, dat_prefix).item()\n",
    "                if score_cb < score_prefix:\n",
    "                    answer_prefix_format = Format_Answer(answer_ob, score_prefix)\n",
    "\n",
    "            output_dict[k][\"answer_T5_answer_with_prefix\"] = answer_prefix_format\n",
    "        k += 1\n",
    "    return output_dict"
   ],
   "metadata": {
    "id": "7Fi55fF5guwp"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Processing"
   ],
   "metadata": {
    "id": "XzTG7maBwh5R"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load the paragraphs from the input file"
   ],
   "metadata": {
    "id": "dkVFJcUln63N"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "with open(file_path, \"rb\") as f:\n",
    "    f_text = f.read()\n",
    "    root = json.loads(f_text)\n",
    "\n",
    "paragraphs = root[\"paragraphs\"]\n",
    "paragraphs_fixed = Fix_Paragraphs(paragraphs)\n",
    "\n",
    "if len(paragraphs_fixed) == 0:\n",
    "    raise Exception(\"No valid paragraph found.\")\n",
    "\n",
    "paragraph_dict = Init_Dictionary(paragraphs_fixed)"
   ],
   "metadata": {
    "id": "uW6nEaosT0cv"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Generate the data"
   ],
   "metadata": {
    "id": "H6YYKBFunpls"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# @title Generate topics\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "topic_dict = Get_Topics(paragraph_dict)\n",
    "\n",
    "stop_time = time.perf_counter()\n",
    "generation_time = stop_time - start_time\n",
    "print(\"Topic generation time: \" + str(generation_time))\n",
    "\n",
    "if verbose:\n",
    "    for topic_key in topic_dict:\n",
    "        print(\"  {}:\".format(topic_key))\n",
    "        print(*topic_dict[topic_key], sep=\"\\n\")"
   ],
   "metadata": {
    "cellView": "form",
    "id": "XRHVx5gWBHH1"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# @title Generate prefixes\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "prefix_dict = Get_Prefixes(topic_dict)\n",
    "\n",
    "stop_time = time.perf_counter()\n",
    "generation_time = stop_time - start_time\n",
    "print(\"Prefix generation time: \" + str(generation_time))\n",
    "\n",
    "if verbose:\n",
    "    for prefix_key in prefix_dict:\n",
    "        print(\"  {}:\".format(prefix_key))\n",
    "        print(*prefix_dict[prefix_key], sep=\"\\n\")"
   ],
   "metadata": {
    "cellView": "form",
    "id": "FcoV2sJcBJe4"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# @title Generate questions\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "question_dict = Get_Questions(paragraph_dict, 2)\n",
    "\n",
    "stop_time = time.perf_counter()\n",
    "generation_time = stop_time - start_time\n",
    "print(\"Question generation time: \" + str(generation_time))\n",
    "\n",
    "if verbose:\n",
    "    for question_key in question_dict:\n",
    "        print(\"  {}:\".format(question_key))\n",
    "        print(*question_dict[question_key], sep=\"\\n\")"
   ],
   "metadata": {
    "cellView": "form",
    "id": "FftTI89xBKvv"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# @title Generate answers\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "answer_dict = Get_Answers(paragraph_dict, question_dict)\n",
    "\n",
    "stop_time = time.perf_counter()\n",
    "generation_time = stop_time - start_time\n",
    "print(\"Answer generation time: \" + str(generation_time))\n",
    "\n",
    "if verbose:\n",
    "    for answer_key in answer_dict:\n",
    "        print(\"  {}:\".format(answer_key))\n",
    "        print(*answer_dict[answer_key], sep=\"\\n\")"
   ],
   "metadata": {
    "cellView": "form",
    "id": "hWdQq__iBLvw"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# @title Generate questions from answers\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "question2_dict = Get_Questions2(paragraph_dict, answer_dict)\n",
    "\n",
    "stop_time = time.perf_counter()\n",
    "generation_time = stop_time - start_time\n",
    "print(\"Question from answer generation time: \" + str(generation_time))\n",
    "\n",
    "if verbose:\n",
    "    for question2_key in question2_dict:\n",
    "        print(\"  {}:\".format(question2_key))\n",
    "        print(*question2_dict[question2_key], sep=\"\\n\")"
   ],
   "metadata": {
    "cellView": "form",
    "id": "bcPluP3bBMj_"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# @title Generate answers to questions from answers\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "answer2_dict = Get_Answers2(paragraph_dict, question2_dict)\n",
    "\n",
    "stop_time = time.perf_counter()\n",
    "generation_time = stop_time - start_time\n",
    "print(\"Answer to question from answer generation time: \" + str(generation_time))\n",
    "\n",
    "if verbose:\n",
    "    for answer2_key in answer2_dict:\n",
    "        print(\"  {}:\".format(answer2_key))\n",
    "        print(*answer2_dict[answer2_key], sep=\"\\n\")"
   ],
   "metadata": {
    "cellView": "form",
    "id": "QWNYUybAxOX7"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# @title Generate declaratives\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "declarative_dict = Get_Declaratives(paragraph_dict, answer2_dict)\n",
    "\n",
    "stop_time = time.perf_counter()\n",
    "generation_time = stop_time - start_time\n",
    "print(\"Declarative generation time: \" + str(generation_time))\n",
    "\n",
    "if verbose:\n",
    "    for declarative_key in declarative_dict:\n",
    "        print(\"  {}:\".format(declarative_key))\n",
    "        print(*declarative_dict[declarative_key], sep=\"\\n\")"
   ],
   "metadata": {
    "cellView": "form",
    "id": "5ASqGCE1zh7j"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# @title Generate closed answers\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "closed_answer_dict = Get_Closed_Answers(paragraph_dict, declarative_dict, None)\n",
    "\n",
    "stop_time = time.perf_counter()\n",
    "generation_time = stop_time - start_time\n",
    "print(\"Closed answer generation time: \" + str(generation_time))\n",
    "\n",
    "if verbose:\n",
    "    for closed_answer_key in closed_answer_dict:\n",
    "        print(\"  {}:\".format(closed_answer_key))\n",
    "        print(*closed_answer_dict[closed_answer_key], sep=\"\\n\")"
   ],
   "metadata": {
    "cellView": "form",
    "id": "sKuMttsc1x9a"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# @title Generate closed answers with prefix\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "prefix_answer_dict = Get_Closed_Answers(paragraph_dict, closed_answer_dict, prefix_dict)\n",
    "\n",
    "stop_time = time.perf_counter()\n",
    "generation_time = stop_time - start_time\n",
    "print(\"Closed answer with prefix generation time: \" + str(generation_time))\n",
    "\n",
    "if verbose:\n",
    "    for prefix_answer_key in prefix_answer_dict:\n",
    "        print(\"  {}:\".format(prefix_answer_key))\n",
    "        print(*prefix_answer_dict[prefix_answer_key], sep=\"\\n\")"
   ],
   "metadata": {
    "cellView": "form",
    "id": "JeNHRbe_4Wjr"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Export to JSON"
   ],
   "metadata": {
    "id": "ApzBQHuknx0W"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "Attach_Generated_Content(paragraph_dict, topic_dict, prefix_dict, prefix_answer_dict)\n",
    "\n",
    "with open(output_path, \"w\") as output:\n",
    "    file = {\"paragraphs\": paragraph_dict}\n",
    "    output.write(json.dumps(file, indent=2))"
   ],
   "metadata": {
    "id": "jfgFYmEQNEvu"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
