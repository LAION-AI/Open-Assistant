{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Install any dependencies\n",
    "!pip install pandas\n",
    "!pip install praw\n",
    "!pip install python-dotenv\n",
    "!pip install pyarrow"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Set the head number to the amount of entries you want to load in minus one\n",
    "ENTRIES_COUNT = 1001"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import praw\n",
    "import os\n",
    "from os.path import join, dirname\n",
    "from dotenv import main\n",
    "\n",
    "# Make sure you create a .env file and fill in all the necessary information in the same folder as this script!\n",
    "main.load_dotenv(join(dirname(os.path.realpath('__file__')), '.env'))\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "   client_id=os.environ.get(\"CLIENT_ID\"),\n",
    "   client_secret=os.environ.get(\"CLIENT_SECRET\"),\n",
    "   user_agent=\"CMV_Scraper\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load the data\n",
    "import tarfile\n",
    "import os.path\n",
    "import json\n",
    "import re\n",
    "from bz2 import BZ2File\n",
    "from urllib import request\n",
    "from io import BytesIO\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "fname = \"cmv.tar.bz2\"\n",
    "url = \"https://chenhaot.com/data/cmv/\" + fname\n",
    "\n",
    "# download if not exists\n",
    "if not os.path.isfile(fname):\n",
    "    f = BytesIO()\n",
    "    with request.urlopen(url) as resp, open(fname, 'wb') as f_disk:\n",
    "        data = resp.read()\n",
    "        f_disk.write(data)  # save to disk too\n",
    "        f.write(data)\n",
    "        f.seek(0)\n",
    "else:\n",
    "    f = open(fname, 'rb')\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#tar = tarfile.open(fileobj=f, mode=\"r:bz2\")\n",
    "tar = tarfile.open(fileobj=f, mode=\"r\")\n",
    "\n",
    "# Extract the file we are interested in\n",
    "\n",
    "train_fname = \"op_task/train_op_data.jsonlist.bz2\"\n",
    "test_fname = \"op_task/heldout_op_data.jsonlist.bz2\"\n",
    "\n",
    "train_bzlist = tar.extractfile(train_fname)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Deserialize the JSON list\n",
    "original_posts_train = [\n",
    "    json.loads(line.decode('utf-8'))\n",
    "    for line in BZ2File(train_bzlist)\n",
    "]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "original_posts_train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the jsonlist file into a dataframe\n",
    "#df = pd.read_json(original_posts_train, orient='list', lines=True)\n",
    "df = pd.DataFrame(original_posts_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Function to check if the posts still exists on reddit\n",
    "def try_get_post(post_id):\n",
    "    try:\n",
    "        submission = reddit.submission(id=post_id)\n",
    "        submission.name\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        return False"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes > sign and the template message at the end of a message\n",
    "def cleanup(cmv_post):\n",
    "    lines = [line for line in cmv_post.splitlines()\n",
    "            if not line.lstrip().startswith(\"&gt;\")\n",
    "            and not line.lstrip().startswith(\"____\")\n",
    "            and not line.lstrip().startswith(\"So go forth and CMV, noble redditors!\")\n",
    "            and \"edit\" not in \" \".join(line.lower().split()[:2])\n",
    "            ]\n",
    "    #print(lines)\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# Create the function that will be handling all the data gathering\n",
    "def get_top_comment_and_clean_data(post_id):\n",
    "    #print(post_id.lstrip(\"t3_\"))\n",
    "    last_author = \"\"\n",
    "    # Grab the post\n",
    "    submission = reddit.submission(id=post_id.lstrip(\"t3_\"))\n",
    "    print(submission)\n",
    "\n",
    "    # Grab the highest rated comment on root layer\n",
    "    submission.submission_type = 'best'\n",
    "    submission.comments.replace_more(limit=0)\n",
    "    replies = list(submission.comments)[0].replies.list()\n",
    "\n",
    "    # Just some variables\n",
    "    responses = pros = cons = []\n",
    "\n",
    "    # If the post author doesn't exist this submission was deleted (submission.deleted doesn't work)\n",
    "    if type(submission.author) == type(None):\n",
    "        last_author = \"[deleted]\"\n",
    "    else:\n",
    "        last_author = submission.author.name\n",
    "\n",
    "    is_pro_argument = False\n",
    "\n",
    "    for comment in replies:\n",
    "\n",
    "        #print(comment)\n",
    "        responses.append(comment.body)\n",
    "\n",
    "        # Sometimes for some reason duplicate entries exist\n",
    "        # Also remove automated message with \"Δ\" in it\n",
    "        if comment.body in responses or \"Δ\" in responses:\n",
    "            continue\n",
    "\n",
    "        comment.body = comment.body.replace(\"[deleted]\",\"\")\n",
    "\n",
    "        # If redditor object doesn't exist, the account is invalid/deleted\n",
    "        if type(comment.author) == type(None):\n",
    "            continue\n",
    "\n",
    "        author = comment.author.name\n",
    "\n",
    "        # Assume that whenever the user changes, they are countering the previous person\n",
    "        if author != last_author:\n",
    "            is_pro_argument = !is_pro_argument\n",
    "\n",
    "\n",
    "        # Add to the respective argument type        \n",
    "        if is_pro_argument:\n",
    "            pros.append(comment.body)\n",
    "        else:\n",
    "            cons.append(comment.body)\n",
    "        \n",
    "        last_author = comment.author.name\n",
    "        \n",
    "    # Pros = arguments for the Title of this post\n",
    "    # Cons = arguments against the title of this post\n",
    "    \n",
    "    return pros, cons, responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.head(100)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "print(f\"Loading in {ENTRIES_COUNT} posts\")\n",
    "dataset = df.head(ENTRIES_COUNT)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# the name column does some weird sh** because dataframes already have a name property, so migrate to a different column name\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "dataset[\"post_id\"] = dataset[\"name\"]\n",
    "warnings.filterwarnings('default')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Reset variables for if we run this multiple times\n",
    "all_pros = []\n",
    "all_names = []\n",
    "all_titles = []\n",
    "all_sources = []\n",
    "\n",
    "# load in our data. this will take a while.\n",
    "\n",
    "for i in range(dataset.shape[0]):\n",
    "    post = dataset.iloc[i]\n",
    "\n",
    "    if type(post) == type(None):\n",
    "        continue\n",
    "\n",
    "    assert(post.post_id != i)\n",
    "\n",
    "    pros, _cons, _response = get_top_comment_and_clean_data(post.post_id)\n",
    "\n",
    "    # if type(post.name) == int:\n",
    "    #     continue\n",
    "    # if type(pros) == int:\n",
    "    #     continue\n",
    "    if post.title == \"[deleted]\":\n",
    "        continue\n",
    "\n",
    "    pros = \" \".join(pros)\n",
    "    pros = pros.replace(\"[deleted]\",\"\")\n",
    "\n",
    "    modified_title = post.title.replace('CMV', \"Change my mind\")\n",
    "    post.selftext = cleanup(post.selftext)\n",
    "    all_titles.append(modified_title + \" \" + post.selftext)\n",
    "    all_pros.append(pros)\n",
    "    all_names.append(post.name)\n",
    "    all_sources.append(f\"https://reddit.com/r/changemyview/comments/{post.post_id}\")\n",
    "    #print(post.title)\n",
    "\n",
    "    print(f\"\\n Loading entry {i}/{dataset.shape[0]}: {modified_title}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Place it all into a Pandas Dataframe\n",
    "clean_df = pd.DataFrame({\n",
    "    \"INSTRUCTION\": all_titles,\n",
    "    \"RESPONSE\": all_pros,\n",
    "    \"SOURCE\": all_sources\n",
    "}, index=all_names\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "clean_df.head(9)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Apache Paquete file\n",
    "\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "table = pa.Table.from_pandas(clean_df)\n",
    "pq.write_table(table,\"output.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Test to see if it was sucessful\n",
    "table = pq.read_table(\"output.parquet\")\n",
    "table.to_pandas()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
