{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c076140f",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24334aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6e6edd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from datasets import load_dataset\n",
    "\n",
    "SBERT_MODEL =  \"all-MiniLM-L6-v2\"\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5f274a",
   "metadata": {},
   "source": [
    "## Data\n",
    "- Load data from red-teaming and prosocial-dialog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1c832e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! git clone https://github.com/anthropics/hh-rlhf.git\n",
    "# ! gzip -d hh-rlhf/red-team-attempts/red_team_attempts.jsonl.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e0dde8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "red_teaming = json.load(open(\"/home/shahul/hh-rlhf/red-team-attempts/red_team_attempts.jsonl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "feb3f9b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration allenai--prosocial-dialog-ebbad39ca08b6d44\n",
      "Found cached dataset json (/home/shahul/.cache/huggingface/datasets/allenai___json/allenai--prosocial-dialog-ebbad39ca08b6d44/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b312f76350f49728a269a09bcef2b5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pro_social_data = load_dataset(\"allenai/prosocial-dialog\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2100a74d",
   "metadata": {},
   "source": [
    "#### prepare dict matching ROT to safety label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8e609f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_rot_safetylabels():\n",
    "    \n",
    "    rots = [item[\"rots\"] for item in pro_social_data[\"train\"]]\n",
    "    safety_annotations = [item[\"safety_label\"] for item in pro_social_data[\"train\"]]\n",
    "    results = {}\n",
    "    for rots,sfty in zip(rots,safety_annotations):\n",
    "        for rot in rots:\n",
    "            if rot not in results.keys():\n",
    "                results[rot] = sfty\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5e7deb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rot_sfty = match_rot_safetylabels()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9352d0db",
   "metadata": {},
   "source": [
    "#### Embedding \n",
    "- Embed ROTs from pro-social dataset and Task description from red-teaming data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15e4f05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vectorizer(model=SBERT_MODEL):\n",
    "     return SentenceTransformer(model)\n",
    "    \n",
    "def vectorize_text(model,texts):\n",
    "    \n",
    "    return model.encode(texts, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e30232f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_vectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac1ac14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_descriptions = [item['task_description'] for item in red_teaming]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cfaed031",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70fe5248dfb84adf890e86d63bb54091",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/3630 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cc62495b38e4fb0a806b7a670290aaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1218 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rot_vector = vectorize_text(model,list(rot_sfty.keys()))\n",
    "task_desp_vector = vectorize_text(model, task_descriptions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b86951d",
   "metadata": {},
   "source": [
    "#### cosine similarity\n",
    "- Cosine similarity between ROTs & task description\n",
    "- Done in batches due to size of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "592d1cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.spatial as sp\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "THRESHOLD = 0.75\n",
    "B_SIZE = 100\n",
    "def get_conversations(rot_vector,task_desp_vector):\n",
    "    \n",
    "    batch_size = 1000\n",
    "    matching_dict = defaultdict(list)\n",
    "    for i in tqdm(range(0,rot_vector.shape[0],B_SIZE)):\n",
    "        cosine_sim = 1 - sp.distance.cdist(rot_vector[i: i+B_SIZE,:], task_desp_vector, 'cosine')\n",
    "        sim_indices = np.argwhere(cosine_sim>=THRESHOLD)\n",
    "        for rot_index, red_index in sim_indices:\n",
    "            matching_dict[rot_index+i].append(red_index)\n",
    "    \n",
    "    return matching_dict    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b66b1e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███████████▊                          | 361/1162 [04:46<10:33,  1.26it/s]"
     ]
    }
   ],
   "source": [
    "matching_dict = get_conversations(rot_vector[:],task_desp_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbee317d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of matching samples: \"sum([len(x) for x in matching_dict.values()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0010238d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Math found for ROT : {list(rot_sfty.keys())[0]} \\n {convert_red([red_teaming[idx] for idx in matching_dict[0]])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e4d318",
   "metadata": {},
   "source": [
    "#### Convert red-teaming data to prosocial format\n",
    "- Red-teaming data is laballed for the whole conversation\n",
    "- Pro-social data is labelled per dialog \n",
    "- The `episode_done` flag indicates the end of a conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5a7ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_red(red_teaming):\n",
    "    \n",
    "    all_conversations = []\n",
    "    for item in red_teaming:\n",
    "        transcript = [dialog for dialog in item[\"transcript\"].split(\"\\n\\n\") if dialog.startswith(('Human',\"Assistant\"))]\n",
    "        for i in range(0,len(transcript)+1,2):\n",
    "            conversation = transcript[i:i+2]\n",
    "            conversation = {dialog.split(\":\")[0].strip(): \":\".join(dialog.split(\":\")[1:]).strip() for dialog in conversation}\n",
    "            if set(conversation.keys()) == set([\"Human\",\"Assistant\"]):\n",
    "                conversation[\"episode_done\"] = i == len(transcript)-2\n",
    "                conversation[\"task_description\"] = item[\"task_description\"]\n",
    "                conversation[\"min_harmlessness_score_transcript\"] = item[\"min_harmlessness_score_transcript\"]\n",
    "                conversation[\"task_descripton_harmlessness_score\"] = item[\"task_descripton_harmlessness_score\"]\n",
    "                all_conversations.append(conversation)\n",
    "            \n",
    "                \n",
    "\n",
    "    return all_conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd783c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "red_teaming_df = convert_red(red_teaming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87735dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_records(red_teaming_df[:100]).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f5fe56",
   "metadata": {},
   "source": [
    "\n",
    "## Inference\n",
    "\n",
    "Now we have \n",
    "- red-teaming data samples with highly similar task description as ROTs in prosocial dataset\n",
    "- the red-teaming data in same format as in prosocial dataset.\n",
    "\n",
    "Next\n",
    "- Use pretrained [safety label classifier]() to infer safety labels for red-teaming conversations\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd00a6bf",
   "metadata": {},
   "source": [
    "#### Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c16b728",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAXLEN = 128\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "class ProSocialDataset(Dataset):\n",
    "    \n",
    "    def __init__(self,dataset,rot):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "        self.sep_token  = self.tokenizer.sep_token\n",
    "        self.dataset = dataset\n",
    "        self.rot = rot\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        \n",
    "        context = self.dataset[idx]\n",
    "        idx_start = idx \n",
    "        end = self.dataset[max(0,idx_start-1)][\"episode_done\"]\n",
    "        while ((not end) and (idx_start>0)):\n",
    "            end = self.dataset[max(0,idx_start-2)][\"episode_done\"]\n",
    "            idx_start -= 1\n",
    "        idx_start = max(0,idx_start)\n",
    "        \n",
    "        prev_context = [f'{self.dataset[i][\"Human\"]}' for i in range(idx_start,idx)]\n",
    "        rots = [self.rot]\n",
    "        context = f'{self.dataset[idx][\"Human\"]}' + self.sep_token + \"\".join(prev_context) + self.sep_token + \"\".join(rots)\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "                    context,\n",
    "                    max_length=MAXLEN, \n",
    "                    add_special_tokens=True,\n",
    "                    truncation=True, \n",
    "                    padding='max_length',\n",
    "                    return_tensors=\"pt\")\n",
    "        \n",
    "        \n",
    "        return encoding\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e52a941",
   "metadata": {},
   "source": [
    "#### Run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b6c7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "np.set_printoptions(precision=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d02a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer,AutoModelForSequenceClassification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c563fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"shahules786/prosocial-classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82b526c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pseudo_label(red_teaming,matching_dict,rot_list):\n",
    "    dfs = []\n",
    "    for i in tqdm(range(100)):\n",
    "        red_teaming_data = convert_red([red_teaming[idx] for idx in matching_dict[i]])\n",
    "        rot = rot_list[i]\n",
    "        if len(red_teaming_data)>1:\n",
    "            pred_dataloader = prepare_dataloader(red_teaming_data,rot)\n",
    "            probs,labels = predict(pred_dataloader)\n",
    "            df_ = prepare_dataframe(red_teaming_data,probs,labels,rot)\n",
    "            dfs.append(df_)\n",
    "    return pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31119c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(pred_dataloader):\n",
    "    \n",
    "    probs,labels = [],[]\n",
    "    for data in pred_dataloader:\n",
    "        pred = softmax(model(data[\"input_ids\"].squeeze(1),data[\"attention_mask\"].squeeze(1)).logits.detach().numpy().astype('float16'),axis=1)\n",
    "        #pred_ids.append(pred.argmax(axis=1))\n",
    "        probs.append(pred.max(axis=1))\n",
    "        labels.append([model.config.id2label.get(pred_id) for pred_id in pred.argmax(axis=1)])\n",
    "    \n",
    "    return np.concatenate(probs),np.concatenate(labels)\n",
    "\n",
    "def prepare_dataframe(red_teaming_data,probs,labels,rot):\n",
    "    \n",
    "    red_teaming_df = pd.DataFrame.from_records(red_teaming_data)\n",
    "    red_teaming_df[\"safety_label\"] = labels\n",
    "    red_teaming_df[\"confidence\"] = probs\n",
    "    red_teaming_df[\"rots\"] = [[rot]] * red_teaming_df.shape[0]\n",
    "    return red_teaming_df\n",
    "\n",
    "def prepare_dataloader(red_teaming_data,rot):\n",
    "    \n",
    "    pred_dataset = ProSocialDataset(red_teaming_data,rot)\n",
    "    pred_dataloader = DataLoader(pred_dataset,batch_size=8)\n",
    "    return pred_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6239df",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = pseudo_label(red_teaming,matching_dict,list(rot_sfty.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8889be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726c8c78",
   "metadata": {},
   "source": [
    "#### Save and upload data to HF hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42703a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"/home/shahul/Data/red_teaming_augmented.json\"\n",
    "output_df.to_json(save_path,orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a323804",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_dataset = load_dataset(\"json\",data_files=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4a7022",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_dataset.push_to_hub(\"shahules786/prosocial_augmented\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16dcf94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OpenAssistant",
   "language": "python",
   "name": "openassistant"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
