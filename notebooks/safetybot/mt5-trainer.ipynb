{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "08238e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    T5ForConditionalGeneration, \n",
    "    T5Tokenizer, \n",
    "    EvalPrediction,\n",
    "    DataCollator,\n",
    "    Trainer,\n",
    "    TrainingArguments)\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedcd7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a07f337b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"t5-base\"\n",
    "model = T5ForConditionalGeneration.from_pretrained(MODEL)\n",
    "tokenizer = T5Tokenizer.from_pretrained(MODEL,padding_side=\"right\",truncation_side=\"right\",model_max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfa81676",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECIAL_TOKENS = {\"context_token\":\"<ctx>\",\"sep_token\":\"<sep>\",\"label_token\":\"<cls>\",\"rot_token\":\"<rot>\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91770a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_special_tokens(tokenizer,model):\n",
    "    for key,value in SPECIAL_TOKENS.items():\n",
    "        setattr(tokenizer,key,value)\n",
    "        tokenizer.add_tokens([value])\n",
    "        setattr(tokenizer,key+\"_id\",tokenizer.encode(value)[0])\n",
    "\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fddcd432",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_special_tokens(tokenizer,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "010605d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL2ID = {\n",
    "    \"__casual__\": \"0\",\n",
    "    \"__needs_caution__\": \"1\",\n",
    "    \"__needs_intervention__\": \"2\",\n",
    "    \"__probably_needs_caution__\": \"3\",\n",
    "    \"__possibly_needs_caution__\": \"4\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b3415776",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SafetyDataset(Dataset):\n",
    "    \n",
    "    def __init__(self,dataset,split,tokenizer,max_len=512):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.split = split\n",
    "        self.dataset = dataset[split]\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label2id = LABEL2ID\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        \n",
    "        \n",
    "        idx_start = idx\n",
    "        end = self.dataset[max(0, idx_start - 1)][\"episode_done\"]\n",
    "        while (not end) and (idx_start > 0):\n",
    "            end = self.dataset[max(0, idx_start - 2)][\"episode_done\"]\n",
    "            idx_start -= 1\n",
    "        idx_start = max(0, idx_start)\n",
    "        context = [f'User: {self.dataset[i][\"context\"]}\\n bot:{self.dataset[i][\"response\"]}' for i in range(idx_start, idx)]\n",
    "        context = self.tokenizer.sep_token.join(context)\n",
    "        rots = self.dataset[idx][\"rots\"]\n",
    "        label = self.label2id[self.dataset[idx][\"safety_label\"]]\n",
    "        input_tokens = self.tokenizer.encode(self.dataset[idx][\"context\"],add_special_tokens=False)\n",
    "        print(self.max_len-len(input_tokens))\n",
    "        context = self.tokenizer.encode(context,\n",
    "                                add_special_tokens=False,\n",
    "                               max_length=self.max_len-len(input_tokens),\n",
    "                               )\n",
    "        rots = self.tokenizer.sep_token.join(rots)\n",
    "        input_ids = input_tokens + [self.tokenizer.context_token_id] + context + [self.tokenizer.eos_token_id]\n",
    "        mask = [1]*len(input_ids) + [0] * (self.max_len-len(input_ids))\n",
    "        target_text = self.tokenizer.label_token + label + self.tokenizer.context_token + rots\n",
    "        decoder_ids = self.tokenizer(target_text,\n",
    "                                add_special_tokens=False,\n",
    "                               max_length=self.max_len,\n",
    "                               )\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\":torch.LongTensor(input_ids),\n",
    "            \"attention_mask\":torch.LongTensor(mask),\n",
    "            \"decoder_input_ids\":torch.LongTensor(decoder_ids[\"input_ids\"]),\n",
    "            \"decoder_attention_mask\":torch.LongTensor(decoder_ids[\"attention_mask\"]),\n",
    "        }\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "23290a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration allenai--prosocial-dialog-ebbad39ca08b6d44\n",
      "Found cached dataset json (/home/shahul/.cache/huggingface/datasets/allenai___json/allenai--prosocial-dialog-ebbad39ca08b6d44/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7003f66e1c3e4666a6b06bbd5de93302",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"allenai/prosocial-dialog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2ffe0a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SafetyDataset(dataset,split=\"train\",tokenizer=tokenizer,max_len=512)\n",
    "valid_dataset = SafetyDataset(dataset,split=\"validation\",tokenizer=tokenizer,max_len=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f179ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This dataclass implementation is taken from Suraj Patil: https://github.com/patil-suraj/question_generation\n",
    "@dataclass\n",
    "class T2TDataCollator():\n",
    "  def __call__(self, batch: List) -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Take a list of samples from a Dataset and collate them into a batch.\n",
    "    Returns:\n",
    "    A dictionary of tensors\n",
    "    \"\"\"\n",
    "    \n",
    "    input_ids = torch.stack([example['input_ids'] for example in batch])\n",
    "    lm_labels = torch.stack([example['decoder_input_ids'] for example in batch])\n",
    "    lm_labels[lm_labels[:, :] == 0] = -100 \n",
    "    attention_mask = torch.stack([example['attention_mask'] for example in batch])\n",
    "    decoder_attention_mask = torch.stack([example['decoder_attention_mask'] for example in batch])\n",
    "    \n",
    "    return {\n",
    "        'input_ids': input_ids, \n",
    "        'attention_mask': attention_mask,\n",
    "        'labels': lm_labels, \n",
    "        'decoder_attention_mask': decoder_attention_mask\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c79d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(output_dir=\"\", \n",
    "                                  per_device_train_batch_size=4, \n",
    "                                  per_device_eval_batch_size=4,\n",
    "                                  gradient_accumulation_steps=16,\n",
    "                                  learning_rate=1e-4, \n",
    "                                  num_train_epochs=1,\n",
    "                                  logging_steps=100,\n",
    "                                  run_name=\"safety-bot\",\n",
    "                                  evaluation_strategy=\"steps\",\n",
    "                                  save_steps=500,\n",
    "                                  report_to=\"wandb\",\n",
    "                                  push_to_hub=False,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35a42e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize our Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    data_collator=T2TDataCollator()\n",
    ")\n",
    "\n",
    "# Training\n",
    "trainer.train()\n",
    "\n",
    "# When training is done, we push the fine-tuned model to the Hub\n",
    "#trainer.push_to_hub(\"t5-end2end-questions-generation\")\n",
    "\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OpenAssistant",
   "language": "python",
   "name": "openassistant"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
