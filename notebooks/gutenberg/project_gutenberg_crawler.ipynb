{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Gutenber Crawler\n",
    "\n",
    "Make sure you read the site's TOS and README.md on how to use the crawler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/LAION-AI/Open-Assistant/blob/notebooks/gutenberg/project_gutenberg_crawler.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment and run below lines to set up if running in colab\n",
    "# !git clone https://github.com/LAION-AI/Open-Assistant.git\n",
    "# %cd Open-Assistant/notebooks/gutenberg\n",
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global settings\n",
    "\n",
    "LANG = \"en\"  # crawl english language books, NOTE: there are a few houndred books with multiple languages such as 'en; es', \n",
    "FOLDER = \"text\"  # save metadata and body of text to this folder\n",
    "CHUNKS = 5  # divide the dataset into this many compressed parquet files if you have less memory\n",
    "STATUS = \"crawled.csv\"  # save the list of downloaded files and their status into this csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required packages\n",
    "import os\n",
    "import io\n",
    "import re\n",
    "import requests\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "try: \n",
    "    from BeautifulSoup import BeautifulSoup\n",
    "except ImportError:\n",
    "    from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np    \n",
    "import pandas as pd\n",
    "\n",
    "from typing import Tuple, Optional, Any"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GutenbergCrawler:\n",
    "    \n",
    "    HEADER = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (compatible; GutenbergCrawler/0.1)\",\n",
    "    }\n",
    "    TIMER = 600  # wait ms between calls\n",
    "    MIRRORS = [\n",
    "        \"http://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/\", \n",
    "        \"https://www.gutenberg.org/dirs/\", \n",
    "        \"http://mirrors.xmission.com/gutenberg/\",\n",
    "        #\"https://gutenberg.pglaf.org/\",  # most likely to get rate limited\n",
    "    ]  # see https://www.gutenberg.org/MIRRORS.ALL for available mirrors\n",
    "    \n",
    "    def __init__(self, folder: Optional[str] = None) -> None:\n",
    "        self.folder = folder\n",
    "        if self.folder is not None:\n",
    "            os.makedirs(self.folder, exist_ok=True) \n",
    "        self.calls = 0\n",
    "        self.last_call = 0\n",
    "    \n",
    "    def _get(self, url: str) -> str:\n",
    "        self.calls += 1\n",
    "        diff = max(0.0, self.TIMER - (time.time() - self.last_call))\n",
    "        if diff:\n",
    "            time.sleep(diff / 1000.0)\n",
    "        data = requests.get(url, headers=self.HEADER)\n",
    "        self.last_call = time.time()\n",
    "        if data.status_code == 404:\n",
    "            return None\n",
    "        try:\n",
    "            return data.content.decode(\"utf-8\")\n",
    "        except UnicodeDecodeError:\n",
    "            try:\n",
    "                return data.content.decode(\"ISO-8859-1\")  # latin-1\n",
    "            except UnicodeDecodeError:\n",
    "                return data.content.decode(\"utf-8\", \"backslashreplace\")\n",
    "    \n",
    "    def catalog(self) -> pd.DataFrame:\n",
    "        try:\n",
    "            csv = pd.read_csv(\"https://www.gutenberg.org/cache/epub/feeds/pg_catalog.csv.gz\", sep=\",\")\n",
    "        except Exception:\n",
    "            raw = self._get(\"https://www.gutenberg.org/cache/epub/feeds/pg_catalog.csv\")\n",
    "            if raw is None:\n",
    "                raise ValueError(\"Catalog CSV file does not exist!\")\n",
    "            csv = pd.read_csv(io.StringIO(raw), sep=\",\")\n",
    "        return csv.loc[csv[\"Type\"] == \"Text\"].reset_index(drop=True)\n",
    "    \n",
    "    def search(self, url: str) -> dict:\n",
    "        \"\"\"Use catalog() instead! Returns dict with book_id: 'book title' pairs for gutenberg.org pages\"\"\"\n",
    "        assert \"/www.gutenberg.org\" in url, \"The URL must be a page at https://www.gutenberg.org/\"\n",
    "        html = self._get(url)\n",
    "        if html is None:\n",
    "            return {}\n",
    "        dom = BeautifulSoup(html, \"html.parser\")\n",
    "        results = {}\n",
    "        for a in dom.find_all(\"a\"):\n",
    "            for elem in re.findall(r'<a href=\\\"/ebooks/(\\d+)\\\">(.+?)</a>', str(a)):\n",
    "                ebook, title = elem\n",
    "                results[int(ebook)] = title.replace(\"\\r<br/>\", \"\\r\\n\") \n",
    "        return results\n",
    "        \n",
    "    def download(self, book: int) -> Optional[str]:\n",
    "        book = int(book)\n",
    "        assert book > 0\n",
    "        mirror = np.random.choice(self.MIRRORS)\n",
    "        if book < 10:\n",
    "            page = f\"0/{book}/\"\n",
    "        else:\n",
    "            page = \"/\".join([char for char in str(book)[:-1]]) + f\"/{book}/\"\n",
    "        url = f\"{mirror}{page}{book}-h/{book}-h.htm\"\n",
    "        return self._get(url)\n",
    "        \n",
    "    def parse(self, book: int, html: str) -> Tuple[Optional[str], Optional[str]]:\n",
    "        book = int(book)\n",
    "        assert book > 0\n",
    "        if html is None:\n",
    "            return None, None\n",
    "        dom = BeautifulSoup(html, \"html.parser\")\n",
    "        if dom is None or dom.title is None or dom.title.string is None or \"404\" in dom.title.string:\n",
    "            return None, None\n",
    "    \n",
    "        meta = \"\"\n",
    "        for pre in dom.select(\"title, pre\"):\n",
    "            meta += str(pre.get_text()).strip()\n",
    "            # remove metadata from dom afterwards\n",
    "            pre.extract()\n",
    "        if re.findall(r'(?i)\\*{2,}[^\\n]+?(?:please.+?copyright|copyrighted.+?project)[^\\n]+?\\*{2,}\\r?\\n', meta):\n",
    "            warnings.warn(f\"Book {book} is copyrighted.\")\n",
    "            return None, None\n",
    "        for img in dom.select(\"img\"):\n",
    "            # add image alt attributes as text\n",
    "            try:\n",
    "                img.insert(0, img[\"alt\"])\n",
    "            except KeyError:\n",
    "                pass\n",
    "        text = str(dom.get_text()).strip()\n",
    "        if re.findall(r'(?i)\\*{2,}[^\\n]+?(?:please.+?copyright|copyrighted.+?project)[^\\n]+?\\*{2,}\\r?\\n', text):\n",
    "            warnings.warn(f\"Book {book} is copyrighted.\")\n",
    "            return None, None\n",
    "        \n",
    "        s = re.split(r'(?i)\\*{2,}[^\\n]+?project gutenberg[^\\n]+?\\*{2,}\\s*[\\r\\n]+', text)  # 49843 \n",
    "        if len(s) > 1:\n",
    "            if len(s) > 3:\n",
    "                warnings.warn(f\"Book {book} is malformed.\")\n",
    "                return None, None\n",
    "            meta += s[0]\n",
    "            return meta, s[1]\n",
    "        return meta, text\n",
    "    \n",
    "    @staticmethod\n",
    "    def pretty(text: Optional[str]) -> str:\n",
    "        if not text:\n",
    "            return \"\"\n",
    "        # attempt to remove transcriber's notes\n",
    "        text = re.sub(r'(?i)(?:\\[|\\b)transcriber[\\'’]?s? notes?\\s*(?:[^\\xa0\\n].*?\\]?(?:\\r?\\n){1,2})+', '', text)\n",
    "        # attempt to remove e-text info\n",
    "        text = re.sub(r'(?i)e-text prepared(?:[^\\xa0]\\(?.+\\)?\\r?\\n{1,3})+(?:\\xa0*\\s*note\\:\\s*(?:.+\\s*\\r{0,2}\\n{1,2}){1,5}\\xa0\\s+)?', '', text)\n",
    "        # standardize line endings\n",
    "        text = \"\\r\\n\".join(text.splitlines())\n",
    "        text = re.sub(r'(\\r\\n){3,}', '\\r\\n\\r\\n\\r\\n', text).strip()\n",
    "        return text\n",
    "    \n",
    "    def _write(self, file: str, content: str) -> None:\n",
    "        path = os.path.join(self.folder, file) if self.folder is not None else file\n",
    "        with open(path, \"w+\", encoding=\"utf-8\") as f:\n",
    "            f.write(content)\n",
    "    \n",
    "    def save(self, book:int) -> bool:\n",
    "        html = self.download(book)\n",
    "        meta, text = self.parse(book, html)\n",
    "        if meta:\n",
    "            self._write(f\"{book}_meta.txt\", meta)\n",
    "        if text:\n",
    "            self._write(f\"{book}_text.txt\", text)\n",
    "        return bool(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc = GutenbergCrawler(FOLDER)  # use text/ folder to save files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get the catalog of ebooks (only text types will be returned)\n",
    "df = gc.catalog()\n",
    "df = df.loc[df[\"Language\"] == LANG]\n",
    "assert len(df), \"No matching items in catalog!\"\n",
    "df = df.sample(frac=1)  # random shuffle\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(STATUS):\n",
    "    crawled = pd.read_csv(STATUS)\n",
    "else:\n",
    "    crawled = pd.DataFrame({\"book\":[], \"success\":[]})\n",
    "print(f\"{len(crawled)} out of {len(df)} items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# NOTE: this will take really long depending on the number of ebooks selected\n",
    "for index, row in df.iterrows():\n",
    "    book = row[\"Text#\"]\n",
    "    if book not in crawled[\"book\"].values:\n",
    "        t = time.time()\n",
    "        print(f\"#{book} {row['Title']} ({row['Language']})\", end=\" \")\n",
    "        if gc.save(book):\n",
    "            print(\"✔️\", end=\" \")\n",
    "            crawled = crawled.append({\"book\": book, \"success\": True}, ignore_index=True)\n",
    "        else:\n",
    "            print(\"❌\", end=\" \")\n",
    "            crawled = crawled.append({\"book\": book, \"success\": False}, ignore_index=True)\n",
    "        print(f\"- {(time.time() - t):.3f}s\")\n",
    "        crawled.to_csv(STATUS, index=False)\n",
    "        if len(crawled) % 25 == 0:\n",
    "            print(f\"▶▶▶ {len(crawled)} done ({int(crawled['success'].sum()) } successful) out of {len(df)} ◀◀◀\")\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add the crawled text files into parquet datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawled = pd.read_csv(STATUS)\n",
    "crawled = crawled.loc[crawled[\"success\"] == True]\n",
    "crawled.rename(columns={\"book\": \"Text#\"}, inplace=True)\n",
    "\n",
    "gc = GutenbergCrawler(FOLDER)\n",
    "df = gc.catalog()\n",
    "df = df.loc[df[\"Language\"] == LANG]\n",
    "\n",
    "print(f\"{len(crawled)} out of {len(df)} ({len(crawled) / len(df) * 100.:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(subset=[\"Text#\"], inplace=True)\n",
    "df = pd.merge(df, crawled, on=[\"Text#\"], how=\"inner\")\n",
    "assert not len(df.loc[df[\"success\"] == False])\n",
    "del crawled\n",
    "df.drop(columns=[\"Type\", \"Language\", \"success\"], inplace=True)\n",
    "df.sort_values(by=\"Text#\", ascending=True, inplace=True)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read(file: str) -> Optional[str]:\n",
    "    result = None\n",
    "    if os.path.exists(file):\n",
    "        with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "            result = f.read()\n",
    "    return result\n",
    "\n",
    "def strip(value: Any) -> str:\n",
    "    return str(value).strip() if value and pd.notna(value) else \"\"\n",
    "\n",
    "\n",
    "for chunk in range(CHUNKS):\n",
    "    n = len(df) // CHUNKS\n",
    "    start, end = chunk * n, (chunk + 1) * n if chunk < CHUNKS - 1 else len(df)\n",
    "    \n",
    "    updated = {col:[] for col in list(df.columns) + [\"Body\"]}\n",
    "    books = df[\"Text#\"].values[start:end]\n",
    "    for book in tqdm(books):\n",
    "        text = read(os.path.join(FOLDER, f\"{book}_text.txt\"))\n",
    "        text = gc.pretty(text)\n",
    "        if not text:\n",
    "            continue\n",
    "        \n",
    "        df_row = df.loc[df[\"Text#\"] == book]\n",
    "        updated[\"Text#\"].append(book)\n",
    "        updated[\"Issued\"].append(pd.to_datetime(df_row[\"Issued\"].values[0], format=\"%Y-%m-%d\", errors=\"coerce\"))\n",
    "        updated[\"Title\"].append(strip(df_row[\"Title\"].values[0]))\n",
    "        updated[\"Authors\"].append(strip(df_row[\"Authors\"].values[0]))\n",
    "        updated[\"Subjects\"].append(strip(df_row[\"Subjects\"].values[0]))\n",
    "        updated[\"LoCC\"].append(strip(df_row[\"LoCC\"].values[0]))\n",
    "        updated[\"Bookshelves\"].append(strip(df_row[\"Bookshelves\"].values[0]))\n",
    "        updated[\"Body\"].append(text)\n",
    "    \n",
    "    updated = pd.DataFrame(updated)  \n",
    "    if CHUNKS == 1:\n",
    "        updated.to_parquet(f\"gutenberg_{LANG}_all.pq\", \n",
    "                           index=False, engine=\"pyarrow\", compression=\"gzip\")\n",
    "    else:\n",
    "        updated.to_parquet(f\"gutenberg_{LANG}_{chunk + 1}_of_{CHUNKS}.pq\", \n",
    "                           index=False, engine=\"pyarrow\", compression=\"gzip\")\n",
    "    del updated\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
